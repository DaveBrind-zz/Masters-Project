{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code trains the entire R Peak Detection algorithm as well as\n",
    "# Test it on a hold out set of three patients that will contain a mix of ECG\n",
    "# Signals. This will allow us to see how well it will perform in practise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# For the LSTM\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, GlobalMaxPool1D, Dense, Dropout, Flatten\n",
    "# For data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Potentially for analysing probability output distribution\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to normalise the ECG signals\n",
    "\n",
    "def normalise(x):\n",
    "    mean = np.mean(x)\n",
    "    x -= mean\n",
    "    std = np.std(x)\n",
    "    x /= std\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set is full of patients: 100, 105, 116, 215, 232\n",
    "# All the other patients can be put into the training set\n",
    "\n",
    "train = [101,103,105,107,109,112,114,118,121,123,200,202,205,208,210,213,215,219,221,223,230]\n",
    "\n",
    "final_x = np.zeros((len(train), 65,10000,6), dtype=np.float32)\n",
    "final_y = np.zeros((len(train), 65, 10000), dtype=np.int32)\n",
    "\n",
    "# Loop over patients and create an appropriate input and label array\n",
    "for o,i in enumerate(train):\n",
    "    \n",
    "    #print(i)\n",
    "    \n",
    "    # Load in the raw ECG as well as the clinician annotated peaks\n",
    "    with open('adb de-noised/{}_de-noised.pkl'.format(i), 'rb') as f:\n",
    "        smoothed = pickle.load(f)\n",
    "    \n",
    "    with open('adb final labels/adb peaks/{}_peaks.pkl'.format(i), 'rb') as f:\n",
    "        peaks = pickle.load(f)\n",
    "        \n",
    "    # We have two leads so extract them first\n",
    "        \n",
    "    lead_1 = np.zeros((smoothed.shape[0],))\n",
    "    lead_2 = np.zeros((smoothed.shape[0],))\n",
    "    \n",
    "    for c,dual_point in enumerate(smoothed):\n",
    "        lead_1[c] = dual_point[0]\n",
    "        lead_2[c] = dual_point[1]\n",
    "        \n",
    "    # We are going to implement chunks of 10000 samples\n",
    "    time_stamp = 10000\n",
    "    \n",
    "    detrend_lead_1 = np.asarray([lead_1[i: i + time_stamp] for i in range(0, len(lead_1), time_stamp)])\n",
    "    detrend_lead_2 = np.asarray([lead_2[i: i + time_stamp] for i in range(0, len(lead_2), time_stamp)])\n",
    "    \n",
    "    patient_x = np.zeros((65,10000,6), dtype=np.float32)\n",
    "    \n",
    "    # Use these knots to fit the cubic splines\n",
    "    knot = np.arange(10,10000, 100)\n",
    "    \n",
    "    for index,chunk in enumerate(detrend_lead_1):\n",
    "        \n",
    "        # First we detrend the denoised ECG signal in order to remove baseline wander\n",
    "        # Do this via fitting a cubic spline then removing this from the original signal\n",
    "        tck_1 = interpolate.splrep(list(range(time_stamp)), detrend_lead_1[index],t=knot, k = 3, task = -1)\n",
    "        new_lead_1 = np.array(np.array(detrend_lead_1[index]) - np.array(interpolate.splev(list(range(time_stamp)), tck_1)), dtype = np.float32)\n",
    "        tck_2 = interpolate.splrep(list(range(time_stamp)), detrend_lead_2[index],t=knot, k = 3, task = -1)\n",
    "        new_lead_2 = np.array(np.array(detrend_lead_2[index]) - np.array(interpolate.splev(list(range(time_stamp)), tck_2)), dtype = np.float32)\n",
    "        smoothed = np.array(normalise(new_lead_1),dtype=np.float32)\n",
    "        smoothed2 = np.array(normalise(new_lead_2),dtype=np.float32)\n",
    "        \n",
    "        # Calculate the derivative and second order derivatives\n",
    "        \n",
    "        deriv = np.array(np.gradient(smoothed), dtype=np.float32)\n",
    "        deriv = np.array(normalise(deriv),dtype=np.float32)\n",
    "        deriv2 = np.array(np.gradient(smoothed2),dtype=np.float32)\n",
    "        deriv2 = np.array(normalise(deriv2),dtype=np.float32)\n",
    "        deriv_2 = np.array(np.gradient(deriv),dtype=np.float32)\n",
    "        deriv_2 = np.array(normalise(deriv_2),dtype=np.float32)\n",
    "        deriv2_2 = np.array(np.gradient(deriv2),dtype=np.float32)\n",
    "        deriv2_2 = np.array(normalise(deriv2_2),dtype=np.float32)\n",
    "        \n",
    "        # Stack all these features together to get a complete input array\n",
    "        x = np.hstack((smoothed,deriv))\n",
    "        x = np.hstack((x,deriv_2))\n",
    "        x = np.hstack((x,smoothed2))\n",
    "        x = np.hstack((x,deriv2))\n",
    "        x = np.hstack((x,deriv2_2))\n",
    "\n",
    "        new_x = np.zeros((10000, 6),dtype=np.float32)\n",
    "\n",
    "        for l in range(smoothed.shape[0]):\n",
    "            temp = np.array([smoothed[l], deriv[l], deriv_2[l], smoothed2[l], deriv2[l], deriv2_2[l]], dtype=np.float32)\n",
    "            new_x[l] = temp\n",
    "            \n",
    "        patient_x[index] = new_x\n",
    "        \n",
    "    y = np.zeros((len(lead_1),))\n",
    "    \n",
    "    # Define a buffer either side of peak to allow this as an R peak location\n",
    "    # This way we have a very small 'region' of R peak allowed locations\n",
    "    buffer = 2\n",
    "    \n",
    "    # Set the labels to true in buffer region\n",
    "    for k in peaks:\n",
    "        y[k - buffer: k + buffer] = 1\n",
    "    \n",
    "    patient_y = np.asarray([y[i: i + time_stamp] for i in range(0, len(y), time_stamp)], dtype=np.int32)\n",
    "    \n",
    "    # Now append these to the total\n",
    "    final_x[o] = patient_x\n",
    "    final_y[o] = patient_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(final_x))\n",
    "#print(len(final_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_x = np.array(final_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(final_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(final_x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape so that we just have all the points with 6 features\n",
    "\n",
    "total = (final_x.shape[0] * final_x.shape[1] * final_x.shape[2])\n",
    "test_x = np.reshape(final_x, (total, final_x.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_x[650000]\n",
    "# Re-shaping has worked so now do same for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_y = np.array(final_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.reshape(final_y, (total,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check what proportion split of the data is R regions compared to normal signals\n",
    "\n",
    "def proportions(y):\n",
    "    test = list(y)\n",
    "    points = test.count(1)\n",
    "    print(points/len(test))\n",
    "    print((1 - points/len(test)))\n",
    "    \n",
    "proportions(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dense model with a relu activation function that performs binary classification indicating if a point\n",
    "# Belongs to the R region or not\n",
    "\n",
    "initializer = \"glorot_uniform\"\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation ='relu', input_shape = (6,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss = 'binary_crossentropy', metrics=[\"binary_accuracy\", \"mae\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split set into train and validation randomly -> large enough dataset that simple splitting will work fine\n",
    "x_train, x_val, y_train, y_val = train_test_split(test_x, test_y, test_size=0.25, shuffle = True)\n",
    "# Check proportions to ensure they are similar\n",
    "proportions(y_train)\n",
    "proportions(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model for 50 epochs using an extremely large batch size due to large input data size\n",
    "history = model.fit(x_train, y_train, verbose=True, batch_size = 50000, validation_data = (x_val, y_val), epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a confusion matrix\n",
    "preds = model.predict(x_val, verbose = True)\n",
    "\n",
    "preds = preds.reshape((preds.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Round the probabilities appropriately\n",
    "preds[preds > 0.5] = 1\n",
    "preds[preds <= 0.5] = 0\n",
    "\n",
    "print(confusion_matrix(y_val,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(final_x[0][0][:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model training history and confusion matrix for future analysis\n",
    "\n",
    "confusion = confusion_matrix(y_val,preds)\n",
    "with open('Dense R Peak Results/Non-Weighted/Confusion.pkl'.format(str(i)), 'wb') as f:\n",
    "    pickle.dump(confusion,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dense R Peak Results/Non-Weighted/History.pkl'.format(str(i)), 'wb') as f:\n",
    "    pickle.dump(history,f)\n",
    "    \n",
    "#model.save(\"RNN R Peak Detection Model_reduce_set.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the process for a test patient to see what the accuracy is looking like\n",
    "\n",
    "# Test set is full of patients: 100, 105, 116, 215, 232\n",
    "# All the other patients can be put into the training set\n",
    "\n",
    "test = [100]\n",
    "\n",
    "test_x = np.zeros((len(test), 65,10000,6), dtype=np.float32)\n",
    "test_y = np.zeros((len(test), 65, 10000), dtype=np.int32)\n",
    "\n",
    "for o,i in enumerate(test):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    with open('adb de-noised/{}_de-noised.pkl'.format(i), 'rb') as f:\n",
    "        smoothed = pickle.load(f)\n",
    "    \n",
    "    with open('adb final labels/adb peaks/{}_peaks.pkl'.format(i), 'rb') as f:\n",
    "        peaks = pickle.load(f)\n",
    "        \n",
    "    lead_1 = np.zeros((smoothed.shape[0],))\n",
    "    lead_2 = np.zeros((smoothed.shape[0],))\n",
    "    \n",
    "    for c,dual_point in enumerate(smoothed):\n",
    "        lead_1[c] = dual_point[0]\n",
    "        lead_2[c] = dual_point[1]\n",
    "        \n",
    "    time_stamp = 10000\n",
    "    \n",
    "    detrend_lead_1 = np.asarray([lead_1[i: i + time_stamp] for i in range(0, len(lead_1), time_stamp)])\n",
    "    detrend_lead_2 = np.asarray([lead_2[i: i + time_stamp] for i in range(0, len(lead_2), time_stamp)])\n",
    "    \n",
    "    patient_x = np.zeros((65,10000,6), dtype=np.float32)\n",
    "    \n",
    "    knot = np.arange(10,10000, 100)\n",
    "    \n",
    "    for index,chunk in enumerate(detrend_lead_1):\n",
    "        \n",
    "        tck_1 = interpolate.splrep(list(range(time_stamp)), detrend_lead_1[index],t=knot, k = 3, task = -1)\n",
    "        new_lead_1 = np.array(np.array(detrend_lead_1[index]) - np.array(interpolate.splev(list(range(time_stamp)), tck_1)), dtype = np.float32)\n",
    "        tck_2 = interpolate.splrep(list(range(time_stamp)), detrend_lead_2[index],t=knot, k = 3, task = -1)\n",
    "        new_lead_2 = np.array(np.array(detrend_lead_2[index]) - np.array(interpolate.splev(list(range(time_stamp)), tck_2)), dtype = np.float32)\n",
    "        smoothed = np.array(normalise(new_lead_1),dtype=np.float32)\n",
    "        smoothed2 = np.array(normalise(new_lead_2),dtype=np.float32)\n",
    "        deriv = np.array(np.gradient(smoothed), dtype=np.float32)\n",
    "        deriv = np.array(normalise(deriv),dtype=np.float32)\n",
    "        deriv2 = np.array(np.gradient(smoothed2),dtype=np.float32)\n",
    "        deriv2 = np.array(normalise(deriv2),dtype=np.float32)\n",
    "        deriv_2 = np.array(np.gradient(deriv),dtype=np.float32)\n",
    "        deriv_2 = np.array(normalise(deriv_2),dtype=np.float32)\n",
    "        deriv2_2 = np.array(np.gradient(deriv2),dtype=np.float32)\n",
    "        deriv2_2 = np.array(normalise(deriv2_2),dtype=np.float32)\n",
    "\n",
    "        x = np.hstack((smoothed,deriv))\n",
    "        x = np.hstack((x,deriv_2))\n",
    "        x = np.hstack((x,smoothed2))\n",
    "        x = np.hstack((x,deriv2))\n",
    "        x = np.hstack((x,deriv2_2))\n",
    "\n",
    "        new_x = np.zeros((10000, 6),dtype=np.float32)\n",
    "\n",
    "        for l in range(smoothed.shape[0]):\n",
    "            temp = np.array([smoothed[l], deriv[l], deriv_2[l], smoothed2[l], deriv2[l], deriv2_2[l]], dtype=np.float32)\n",
    "            new_x[l] = temp\n",
    "            \n",
    "        patient_x[index] = new_x\n",
    "        \n",
    "    y = np.zeros((len(lead_1),))\n",
    "\n",
    "    buffer = 3\n",
    "    \n",
    "    # Set the labels to true in buffer region\n",
    "    for k in peaks:\n",
    "        y[k - buffer: k + buffer] = 1\n",
    "    \n",
    "    patient_y = np.asarray([y[i: i + time_stamp] for i in range(0, len(y), time_stamp)], dtype=np.int32)\n",
    "    \n",
    "    # Now append these to the total\n",
    "    test_x[o] = patient_x\n",
    "    test_y[o] = patient_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.array(test_x).reshape(650000,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array(test_y).reshape(650000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_x, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.reshape((pred.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(new_prob_peaks, peaks):\n",
    "\n",
    "    sample_window = 2\n",
    "    counts = 0\n",
    "\n",
    "    temp_list = list(new_prob_peaks)\n",
    "\n",
    "    # Loop over the true annotated peaks as well as the predicted peaks\n",
    "    for true_peak in peaks:\n",
    "        for predicted_peak in temp_list:\n",
    "\n",
    "            # If they are within the sample window either side of the true peak then allow it as a count for our case\n",
    "            if ((predicted_peak >= (true_peak - sample_window)) and (predicted_peak <= (true_peak + sample_window))):\n",
    "                counts += 1\n",
    "                # Now remove this peak from the list as it is one-to-one relation so cannot be used again\n",
    "                temp_list.remove(predicted_peak)\n",
    "                # Now break from the for loop so we do not do the other components\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "    accuracy = (counts/len(new_prob_peaks))\n",
    "    missing = abs(len(peaks) - round(accuracy*len(new_prob_peaks)))\n",
    "    wrong = (missing + round((1-accuracy) * len(new_prob_peaks)))\n",
    "    return wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_peaks, _ = find_peaks(pred, height = 0.05, distance = 10)\n",
    "new_prob_peaks = prob_peaks[lead_1[prob_peaks] > 0]\n",
    "accuracy = accuracy_metric(new_prob_peaks, peaks)\n",
    "#print(accuracy)\n",
    "\n",
    "#print(len(new_prob_peaks))\n",
    "\n",
    "#print(len(peaks))\n",
    "\n",
    "# Plot results\n",
    "plt.plot(lead_1[:11000])\n",
    "temp = new_prob_peaks[:41]\n",
    "plt.plot(temp, lead_1[temp], \"x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
