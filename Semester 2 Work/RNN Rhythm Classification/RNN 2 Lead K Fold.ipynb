{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Code Outline -------- #\n",
    "# Create a model that implements an RNN using output predictions from a CNN in order to identify rhythm changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650000\n"
     ]
    }
   ],
   "source": [
    "with open('adb de-noised/100_de-noised.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "end = len(y)\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will have time series data of labels for each patient. These series will be different lengths so we need to\n",
    "# Pad them all to add 0's to the end of each sequence to get longer length inputs. So first we save how many labels are\n",
    "# In each patient as well as how many beats are in the signal.\n",
    "\n",
    "# Load in the beat labels for each patient as well as the rhythm labels for each\n",
    "\n",
    "patient_id = range(0,48)\n",
    "\n",
    "beat_labels = []\n",
    "\n",
    "beat_locations = []\n",
    "\n",
    "rhythm_labels = []\n",
    "\n",
    "rhythm_locations = []\n",
    "\n",
    "full_test = [14,24]\n",
    "half_test = [4,28,40,41,44,45]\n",
    "\n",
    "test_beat_labels = []\n",
    "test_beat_locations = []\n",
    "test_rhythm_labels = []\n",
    "test_rhythm_locations = []\n",
    "\n",
    "half_beat_labels = []\n",
    "half_beat_locations = []\n",
    "half_rhythm_labels = []\n",
    "half_rhythm_locations = []\n",
    "\n",
    "for i in patient_id:\n",
    "    # Beat labels for patient i\n",
    "    with open('adb final labels/adb beat labels/{}_beat_labels.pkl'.format(i), 'rb') as f:\n",
    "        temp_beat = pickle.load(f)\n",
    "    # beat label locations for patient i\n",
    "    with open('adb final labels/adb peaks/{}_peaks.pkl'.format(i), 'rb') as f:\n",
    "        temp_beat_locations = pickle.load(f)\n",
    "    # rhythm labels for patient i\n",
    "    with open('adb final labels/adb rhythm labels/{}_rhythm_labels.pkl'.format(i), 'rb') as f:\n",
    "        temp_rhythm = pickle.load(f)\n",
    "    # Rhythm label locations for patient i\n",
    "    with open('adb final labels/adb rhythm locations/{}_rhythm_locations.pkl'.format(i), 'rb') as f:\n",
    "        temp_rhythm_location = pickle.load(f)\n",
    "    if(i in full_test):\n",
    "        test_rhythm_locations.append(temp_rhythm_location)\n",
    "        test_beat_labels.append(temp_beat)\n",
    "        test_rhythm_labels.append(temp_rhythm)\n",
    "        test_beat_locations.append(temp_beat_locations)\n",
    "    elif(i in half_test):\n",
    "        half_rhythm_locations.append(temp_rhythm_location)\n",
    "        half_beat_labels.append(temp_beat)\n",
    "        half_rhythm_labels.append(temp_rhythm)\n",
    "        half_beat_locations.append(temp_beat_locations)\n",
    "    else:\n",
    "        rhythm_locations.append(temp_rhythm_location)\n",
    "        beat_labels.append(temp_beat)\n",
    "        rhythm_labels.append(temp_rhythm)\n",
    "        beat_locations.append(temp_beat_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "['SBR']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(rhythm_labels))\n",
    "print(half_rhythm_labels[-1])\n",
    "print(len(test_rhythm_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2209\n"
     ]
    }
   ],
   "source": [
    "print(len(beat_labels[32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1116]\n",
      "[1116, 1121]\n",
      "[1116, 1121, 1148]\n",
      "[1116, 1121, 1148, 1293]\n",
      "[1116, 1121, 1148, 1293, 779]\n",
      "[1116, 1121, 1148, 1293, 779, 914]\n"
     ]
    }
   ],
   "source": [
    "# Now split the half patient test data into half in order to get representative test set\n",
    "index = []\n",
    "for j in half_beat_locations:\n",
    "    # Find beat location nearest to the halfway point\n",
    "    index.append(min(enumerate(j), key=lambda x: abs(x[1]-(end/2)))[0])\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_beat_to_rhythm(beat_location, rhythm_location):\n",
    "    index, value = min(enumerate(rhythm_location), key=lambda x: abs(x[1]-(beat_location)))\n",
    "    return index, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have found the beat corresponding to the halfway point of the ecg, and then the function above finds the last rhythm\n",
    "# Label before this point\n",
    "\n",
    "# For every halfway beat in the list above we now need to find the nearest rhythm to it and use that as our starting rhythm\n",
    "\n",
    "for c,k in enumerate(index):\n",
    "    starting_rhythm_index, starting_rhythm_value = find_beat_to_rhythm(half_beat_locations[c][k], half_rhythm_locations[c])\n",
    "    halfway_list_locations = [end / 2]\n",
    "    halfway_list_labels = [half_rhythm_labels[c][starting_rhythm_index]]\n",
    "    \n",
    "    if(c == 0):\n",
    "        # We want left half of patient ECG in the test set\n",
    "        test_rhythm_labels.append(half_rhythm_labels[c][:starting_rhythm_index])\n",
    "        temp_list = half_rhythm_locations[c][:starting_rhythm_index] + [end / 2]\n",
    "        test_rhythm_locations.append(temp_list)\n",
    "        test_beat_labels.append(half_beat_labels[c][:k])\n",
    "        test_beat_locations.append(half_beat_locations[c][:k])\n",
    "        # Save the right half into the training set\n",
    "        temp_labels = halfway_list_labels + half_rhythm_labels[c][(starting_rhythm_index):]\n",
    "        rhythm_labels.append(temp_labels)\n",
    "        temp_locations = halfway_list_locations + half_rhythm_locations[c][(starting_rhythm_index):]\n",
    "        rhythm_locations.append(temp_locations)\n",
    "        beat_labels.append(half_beat_labels[c][k:])\n",
    "        beat_locations.append(half_beat_locations[c][k:])\n",
    "    else:\n",
    "        # We want right half of patient ECG in the test set\n",
    "        test_rhythm_labels.append(half_rhythm_labels[c][(starting_rhythm_index - 1):])\n",
    "        temp_list =  [end / 2] + half_rhythm_locations[c][starting_rhythm_index:]\n",
    "        test_rhythm_locations.append(temp_list)\n",
    "        test_beat_labels.append(half_beat_labels[c][k:])\n",
    "        test_beat_locations.append(half_beat_locations[c][k:])\n",
    "        # Save the left half into the training set\n",
    "        if(len(half_rhythm_labels[c]) == 1): # If we only have one rhythm present in the sample\n",
    "            temp_labels = [half_rhythm_labels[c][0]]\n",
    "            temp_locations = [half_rhythm_locations[c][0]]\n",
    "        else:\n",
    "            temp_labels = half_rhythm_labels[c][:starting_rhythm_index]\n",
    "            temp_locations = half_rhythm_locations[c][:starting_rhythm_index]\n",
    "        rhythm_labels.append(temp_labels)\n",
    "        rhythm_locations.append(temp_locations)\n",
    "        beat_labels.append(half_beat_labels[c][:k])\n",
    "        beat_locations.append(half_beat_locations[c][:k])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324899\n"
     ]
    }
   ],
   "source": [
    "print(half_beat_locations[0][index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 316, 615, 901, 1188, 1477, 1749, 2062, 2361, 2651, 2942, 3232, 3522, 3796, 4102, 4407, 4704, 4995, 5281, 5569, 5873, 6143, 6451, 6744, 7042, 7332, 7619, 7921, 8184, 8496, 8794, 9088, 9378, 9678, 9951, 10232, 10533, 10839, 11135, 11434, 11725, 12040, 12287, 12575, 12885, 13185, 13479, 13783, 14059, 14339, 14617, 14930, 15233, 15525, 15816, 16118, 16390, 16671, 16967, 17274, 17571, 17869, 18171, 18444, 18724, 19008, 19318, 19616, 19910, 20189, 20492, 20804, 21056, 21362, 21668, 21959, 22249, 22532, 22828, 23101, 23398, 23701, 23998, 24300, 24590, 24877, 25178, 25440, 25747, 26048, 26341, 26634, 26926, 27184, 27489, 27778, 28084, 28380, 28675, 28963, 29250, 29519, 29812, 30144, 30421, 30719, 31012, 31302, 31580, 31861, 32168, 32445, 32734, 33059, 33332, 33618, 33896, 34194, 34498, 34794, 35091, 35359, 35661, 35938, 36225, 36529, 36827, 37123, 37417, 37709, 37977, 38278, 38574, 38872, 39165, 39462, 39754, 40039, 40302, 40607, 40910, 41210, 41511, 41802, 42136, 42372, 42651, 42952, 43244, 43556, 43843, 44140, 44421, 44698, 45002, 45306, 45600, 45897, 46180, 46456, 46745, 47033, 47340, 47636, 47938, 48232, 48526, 48806, 49085, 49391, 49684, 49934, 50247, 50557, 50820, 51097, 51394, 51697, 51994, 52295, 52587, 52875, 53154, 53436, 53739, 54035, 54295, 54584, 54879, 55173, 55464, 55776, 56050, 56348, 56644, 56938, 57226, 57504, 57791, 58094, 58391, 58669, 58951, 59265, 59536, 59816, 60122, 60421, 60712, 61011, 61307, 61589, 61855, 62126, 62458, 62737, 63040, 63333, 63623, 63905, 64188, 64487, 64785, 65057, 65335, 65652, 65950, 66205, 66508, 66808, 67099, 67398, 67695, 67979, 68260, 68552, 68848, 69131, 69450, 69704, 69994, 70279, 70562, 70867, 71170, 71472, 71770, 72073, 72360, 72635, 72934, 73231, 73527, 73830, 74120, 74409, 74680, 74961, 75259, 75552, 75849, 76146, 76428, 76700, 76968, 77257, 77548, 77838, 78133, 78425, 78707, 78983, 79260, 79554, 79845, 80136, 80430, 80721, 80999, 81267, 81553, 81845, 82134, 82323, 82638, 82932, 83209, 83488, 83787, 84081, 84377, 84677, 84976, 85260, 85536, 85830, 86127, 86423, 86721, 87020, 87310, 87587, 87874, 88171, 88491, 88793, 89085, 89380, 89655, 89927, 90223, 90517, 90807, 91107, 91408, 91693, 91969, 92267, 92507, 92817, 93118, 93416, 93712, 93994, 94270, 94567, 94861, 95161, 95462, 95764, 96048, 96325, 96621, 96913, 97204, 97505, 97805, 98093, 98370, 98661, 98961, 99248, 99553, 99853, 100116, 100349, 100624, 100902, 101187, 101478, 101765, 102062, 102348, 102616, 102893, 103176, 103463, 103751, 104041, 104339, 104612, 104881, 105172, 105449, 105741, 106039, 106329, 106625, 106898, 107175, 107473, 107764, 108057, 108362, 108665, 108943, 109223, 109516, 109809, 110106, 110398, 110702, 110997, 111269, 111566, 111856, 112150, 112456, 112756, 113047, 113327, 113612, 113908, 114196, 114505, 114816, 115121, 115405, 115658, 115955, 116246, 116567, 116840, 117144, 117433, 117710, 117999, 118291, 118582, 118898, 119185, 119487, 119763, 120047, 120341, 120627, 120925, 121228, 121530, 121816, 122092, 122387, 122676, 122965, 123264, 123571, 123865, 124145, 124435, 124722, 125012, 125315, 125615, 125918, 126199, 126480, 126773, 127060, 127354, 127657, 127965, 128251, 128528, 128838, 129106, 129379, 129666, 129950, 130236, 130502, 130770, 131056, 131330, 131611, 131901, 132201, 132487, 132761, 133047, 133334, 133623, 133918, 134218, 134526, 134807, 135087, 135386, 135677, 135976, 136284, 136586, 136880, 137159, 137458, 137746, 138037, 138306, 138611, 138909, 139189, 139477, 139767, 140053, 140354, 140656, 140960, 141269, 141546, 141833, 142113, 142398, 142695, 143002, 143291, 143562, 143856, 144143, 144433, 144733, 145047, 145347, 145635, 145937, 146197, 146484, 146784, 147086, 147392, 147677, 147959, 148250, 148533, 148826, 149129, 149437, 149726, 150005, 150301, 150585, 150869, 151166, 151476, 151739, 151994, 152286, 152576, 152862, 153156, 153456, 153765, 154062, 154348, 154644, 154935, 155201, 155500, 155813, 156113, 156392, 156687, 156971, 157250, 157546, 157850, 158152, 158439, 158700, 158986, 159278, 159587, 159903, 160188, 160478, 160760, 161052, 161335, 161618, 161917, 162226, 162527, 162809, 163101, 163390, 163671, 163930, 164223, 164538, 164830, 165116, 165413, 165702, 165972, 166271, 166581, 166881, 167160, 167455, 167740, 168020, 168315, 168621, 168923, 169211, 169499, 169785, 170066, 170361, 170628, 170911, 171215, 171499, 171791, 172071, 172355, 172652, 172962, 173264, 173561, 173855, 174145, 174397, 174691, 174995, 175302, 175592, 175880, 176168, 176447, 176738, 177038, 177347, 177641, 177925, 178185, 178462, 178741, 179042, 179357, 179667, 179955, 180258, 180522, 180799, 181091, 181391, 181696, 182017, 182278, 182565, 182844, 183131, 183430, 183740, 184006, 184280, 184586, 184873, 185159, 185452, 185770, 186047, 186340, 186634, 186920, 187199, 187489, 187787, 188094, 188389, 188677, 188969, 189246, 189529, 189820, 190077, 190388, 190692, 190994, 191283, 191567, 191831, 192132, 192440, 192734, 193025, 193317, 193592, 193879, 194177, 194487, 194784, 195048, 195345, 195641, 195930, 196202, 196509, 196811, 197103, 197400, 197685, 197962, 198251, 198549, 198856, 199154, 199415, 199710, 200000, 200294, 200598, 200882, 201180, 201471, 201771, 202052, 202325, 202613, 202918, 203222, 203515, 203814, 204078, 204339, 204635, 204949, 205260, 205532, 205827, 206122, 206400, 206684, 206975, 207283, 207584, 207874, 208175, 208460, 208731, 208997, 209282, 209598, 209899, 210217, 210483, 210760, 211047, 211338, 211644, 211941, 212235, 212532, 212810, 213088, 213375, 213664, 213951, 214268, 214587, 214847, 215122, 215406, 215702, 216009, 216306, 216604, 216896, 217171, 217442, 217713, 218046, 218357, 218624, 218928, 219208, 219485, 219769, 220071, 220376, 220669, 220974, 221256, 221539, 221840, 222114, 222420, 222718, 223019, 223315, 223592, 223875, 224162, 224469, 224744, 225040, 225360, 225662, 225911, 226197, 226496, 226798, 227094, 227400, 227689, 227967, 228249, 228537, 228826, 229117, 229426, 229743, 230004, 230283, 230572, 230876, 231177, 231472, 231778, 232062, 232333, 232615, 232912, 233197, 233483, 233804, 234114, 234371, 234655, 234943, 235243, 235542, 235844, 236140, 236421, 236704, 236987, 237279, 237584, 237896, 238192, 238480, 238752, 239035, 239332, 239630, 239925, 240236, 240530, 240798, 241081, 241391, 241669, 241966, 242272, 242573, 242852, 243136, 243420, 243718, 244017, 244295, 244612, 244924, 245176, 245459, 245756, 246056, 246351, 246664, 246958, 247232, 247513, 247777, 248083, 248408, 248684, 248986, 249268, 249551, 249836, 250135, 250430, 250726, 251037, 251323, 251570, 251875, 252153, 252450, 252747, 253055, 253356, 253636, 253918, 254204, 254501, 254794, 255090, 255407, 255708, 255968, 256253, 256545, 256840, 257136, 257449, 257743, 258018, 258296, 258593, 258883, 259180, 259492, 259790, 260072, 260353, 260634, 260931, 261225, 261484, 261808, 262095, 262375, 262659, 262952, 263243, 263539, 263850, 264137, 264434, 264700, 264992, 265285, 265578, 265889, 266193, 266477, 266759, 267042, 267326, 267619, 267943, 268234, 268521, 268807, 269089, 269383, 269672, 269966, 270283, 270578, 270856, 271107, 271405, 271702, 272007, 272298, 272599, 272887, 273169, 273450, 273742, 274035, 274338, 274646, 274937, 275200, 275490, 275805, 276073, 276368, 276681, 276977, 277258, 277540, 277829, 278117, 278405, 278717, 279044, 279309, 279595, 279878, 280165, 280453, 280756, 281064, 281356, 281645, 281928, 282221, 282508, 282799, 283116, 283416, 283695, 283978, 284268, 284556, 284840, 285157, 285461, 285751, 286035, 286318, 286606, 286891, 287191, 287512, 287792, 288084, 288367, 288655, 288940, 289232, 289547, 289846, 290119, 290407, 290718, 290989, 291275, 291589, 291894, 292180, 292463, 292747, 293033, 293292, 293608, 293937, 294205, 294501, 294783, 295072, 295353, 295643, 295956, 296253, 296521, 296822, 297107, 297393, 297676, 297988, 298295, 298584, 298869, 299157, 299433, 299730, 300024, 300334, 300628, 300922, 301207, 301494, 301774, 302067, 302382, 302670, 302967, 303251, 303543, 303825, 304108, 304420, 304724, 305037, 305301, 305591, 305877, 306158, 306460, 306767, 307063, 307360, 307638, 307928, 308207, 308498, 308806, 309101, 309401, 309682, 309984, 310261, 310544, 310854, 311156, 311448, 311737, 312028, 312298, 312585, 312918, 313198, 313492, 313791, 314078, 314365, 314643, 314934, 315234, 315545, 315833, 316125, 316417, 316695, 316975, 317286, 317587, 317872, 318168, 318484, 318748, 319027, 319326, 319631, 319927, 320221, 320510, 320804, 321098, 321368, 321674, 321969, 322269, 322561, 322851, 323133, 323426, 323714, 324012, 324307, 324603, 324899, 325179, 325469, 325769, 326059, 326354, 326650, 326945, 327232, 327508, 327809, 328115, 328397, 328698, 328991, 329283, 329559, 329844, 330150, 330459, 330740, 331038, 331332, 331612, 331888, 332189, 332485, 332780, 333075, 333358, 333655, 333933, 334228, 334525, 334812, 335114, 335438, 335708, 335986, 336269, 336568, 336862, 337162, 337453, 337757, 338057, 338315, 338614, 338908, 339204, 339502, 339803, 340088, 340364, 340677, 340957, 341253, 341554, 341854, 342145, 342422, 342709, 343001, 343314, 343600, 343900, 344200, 344479, 344755, 345056, 345351, 345646, 345944, 346247, 346530, 346806, 347102, 347395, 347689, 347994, 348286, 348581, 348861, 349146, 349440, 349734, 350035, 350336, 350636, 350916, 351196, 351490, 351779, 352075, 352374, 352676, 352964, 353239, 353536, 353827, 354115, 354416, 354723, 354993, 355284, 355571, 355861, 356153, 356456, 356755, 357057, 357337, 357618, 357900, 358201, 358525, 358801, 359107, 359389, 359666, 359962, 360247, 360538, 360810, 361132, 361421, 361699, 361992, 362281, 362566, 362866, 363169, 363461, 363752, 364062, 364326, 364613, 364913, 365215, 365525, 365813, 366089, 366382, 366666, 366953, 367257, 367564, 367857, 368137, 368431, 368720, 368999, 369300, 369606, 369907, 370191, 370477, 370765, 371053, 371347, 371652, 371958, 372245, 372526, 372819, 373104, 373395, 373719, 374005, 374298, 374576, 374873, 375158, 375439, 375724, 376043, 376340, 376627, 376918, 377204, 377488, 377785, 378086, 378378, 378673, 378981, 379253, 379531, 379822, 380123, 380434, 380730, 381011, 381286, 381593, 381859, 382156, 382466, 382766, 383049, 383343, 383628, 383891, 384194, 384524, 384805, 385097, 385386, 385673, 385954, 386247, 386546, 386861, 387147, 387432, 387726, 388007, 388286, 388584, 388895, 389199, 389480, 389779, 390063, 390339, 390627, 390933, 391238, 391532, 391820, 392107, 392385, 392676, 392974, 393282, 393582, 393889, 394160, 394439, 394723, 395017, 395331, 395629, 395911, 396222, 396494, 396769, 397061, 397367, 397670, 397964, 398262, 398549, 398840, 399108, 399407, 399714, 400009, 400306, 400596, 400874, 401149, 401460, 401764, 402064, 402353, 402651, 402934, 403208, 403496, 403806, 404091, 404397, 404699, 404985, 405259, 405545, 405843, 406147, 406442, 406741, 407030, 407327, 407592, 407884, 408194, 408491, 408788, 409085, 409360, 409637, 409928, 410238, 410540, 410832, 411136, 411421, 411692, 411985, 412302, 412581, 412874, 413181, 413469, 413744, 414028, 414329, 414620, 414920, 415220, 415514, 415793, 416077, 416369, 416670, 416968, 417265, 417570, 417852, 418127, 418412, 418707, 419006, 419304, 419617, 419906, 420179, 420459, 420755, 421055, 421334, 421658, 421970, 422226, 422511, 422794, 423095, 423396, 423695, 423997, 424261, 424558, 424835, 425139, 425437, 425732, 426044, 426329, 426602, 426872, 427179, 427473, 427772, 428080, 428377, 428655, 428937, 429224, 429522, 429821, 430122, 430426, 430706, 430990, 431274, 431571, 431868, 432180, 432474, 432767, 433041, 433323, 433618, 433912, 434213, 434537, 434818, 435095, 435375, 435661, 435956, 436252, 436569, 436862, 437149, 437431, 437713, 438008, 438301, 438603, 438921, 439196, 439479, 439760, 440055, 440349, 440644, 440958, 441254, 441528, 441809, 442101, 442390, 442683, 442997, 443297, 443562, 443871, 444138, 444431, 444724, 445030, 445336, 445626, 445911, 446179, 446484, 446767, 447066, 447379, 447673, 447956, 448238, 448528, 448816, 449105, 449441, 449723, 450006, 450287, 450575, 450859, 451147, 451461, 451753, 452059, 452339, 452622, 452911, 453200, 453500, 453809, 454103, 454393, 454693, 454966, 455249, 455542, 455858, 456154, 456440, 456724, 457022, 457298, 457584, 457898, 458199, 458487, 458772, 459062, 459358, 459629, 459936, 460243, 460539, 460833, 461115, 461409, 461684, 461978, 462293, 462590, 462879, 463164, 463456, 463738, 464021, 464340, 464642, 464931, 465217, 465503, 465788, 466051, 466384, 466675, 466971, 467263, 467547, 467833, 468116, 468412, 468698, 469008, 469300, 469587, 469878, 470159, 470443, 470755, 471054, 471342, 471610, 471927, 472194, 472475, 472784, 473085, 473378, 473670, 473959, 474231, 474514, 474816, 475123, 475421, 475720, 476008, 476297, 476575, 476860, 477179, 477466, 477764, 478055, 478347, 478627, 478905, 479214, 479512, 479804, 480102, 480396, 480676, 480954, 481257, 481557, 481862, 482146, 482439, 482727, 483006, 483296, 483599, 483905, 484208, 484492, 484785, 485063, 485346, 485646, 485941, 486244, 486534, 486831, 487113, 487389, 487691, 487989, 488282, 488573, 488896, 489165, 489443, 489738, 490035, 490332, 490633, 490904, 491211, 491484, 491769, 492071, 492364, 492665, 492965, 493263, 493541, 493798, 494114, 494405, 494700, 494999, 495298, 495585, 495860, 496157, 496453, 496725, 497047, 497336, 497629, 497909, 498194, 498486, 498780, 499083, 499375, 499681, 499984, 500241, 500537, 500828, 501123, 501426, 501730, 502012, 502292, 502606, 502872, 503163, 503464, 503771, 504063, 504340, 504634, 504939, 505213, 505513, 505816, 506113, 506392, 506678, 506967, 507257, 507558, 507859, 508166, 508446, 508725, 509021, 509305, 509599, 509922, 510208, 510495, 510773, 511069, 511353, 511640, 511938, 512247, 512544, 512826, 513116, 513406, 513690, 513988, 514292, 514573, 514880, 515155, 515443, 515725, 516023, 516325, 516633, 516923, 517198, 517471, 517778, 518048, 518349, 518661, 518957, 519239, 519533, 519818, 520093, 520391, 520723, 521007, 521294, 521582, 521871, 522151, 522446, 522750, 523060, 523370, 523631, 523924, 524205, 524493, 524792, 525103, 525396, 525690, 525974, 526258, 526536, 526831, 527141, 527442, 527732, 528017, 528334, 528590, 528880, 529183, 529492, 529783, 530070, 530357, 530653, 530926, 531227, 531534, 531828, 532119, 532409, 532681, 532969, 533264, 533578, 533876, 534162, 534458, 534741, 535017, 535288, 535624, 535912, 536204, 536503, 536788, 537065, 537350, 537649, 537927, 538240, 538521, 538815, 539093, 539378, 539673, 539981, 540278, 540554, 540851, 541153, 541417, 541704, 542011, 542315, 542607, 542911, 543195, 543458, 543763, 544053, 544360, 544656, 544957, 545246, 545521, 545806, 546095, 546422, 546700, 546998, 547294, 547571, 547854, 548145, 548463, 548769, 549044, 549348, 549631, 549906, 550188, 550491, 550808, 551087, 551397, 551688, 551961, 552243, 552538, 552842, 553157, 553442, 553736, 554014, 554298, 554583, 554887, 555183, 555504, 555787, 556067, 556347, 556631, 556932, 557231, 557513, 557840, 558122, 558397, 558675, 558971, 559275, 559570, 559879, 560153, 560449, 560720, 561008, 561304, 561602, 561907, 562205, 562486, 562771, 563029, 563341, 563632, 563930, 564239, 564527, 564806, 565088, 565384, 565678, 565951, 566289, 566573, 566848, 567127, 567419, 567713, 568006, 568318, 568620, 568905, 569181, 569466, 569756, 570049, 570355, 570658, 570946, 571226, 571517, 571806, 572100, 572397, 572710, 573002, 573282, 573564, 573856, 574171, 574441, 574757, 575054, 575334, 575616, 575903, 576191, 576499, 576793, 577099, 577385, 577670, 577954, 578242, 578521, 578827, 579142, 579437, 579727, 580009, 580302, 580588, 580879, 581181, 581490, 581793, 582056, 582350, 582634, 582923, 583241, 583543, 583830, 584087, 584381, 584666, 584953, 585259, 585566, 585859, 586149, 586433, 586721, 586986, 587287, 587624, 587899, 588192, 588476, 588768, 589051, 589336, 589651, 589936, 590218, 590538, 590805, 591089, 591371, 591684, 591986, 592276, 592566, 592849, 593153, 593416, 593718, 594026, 594323, 594619, 594903, 595192, 595474, 595784, 596071, 596368, 596666, 596953, 597244, 597523, 597801, 598121, 598415, 598707, 599000, 599294, 599574, 599853, 600160, 600460, 600779, 601047, 601337, 601623, 601903, 602198, 602503, 602797, 603096, 603411, 603681, 603959, 604246, 604549, 604844, 605145, 605439, 605714, 606008, 606283, 606588, 606886, 607182, 607480, 607778, 608059, 608336, 608632, 608936, 609254, 609527, 609824, 610112, 610389, 610679, 610981, 611278, 611580, 611869, 612169, 612467, 612729, 613028, 613321, 613622, 613922, 614219, 614499, 614768, 615074, 615370, 615666, 615965, 616267, 616552, 616828, 617125, 617418, 617713, 618028, 618311, 618604, 618882, 619170, 619465, 619758, 620057, 620348, 620673, 620933, 621216, 621507, 621794, 622100, 622402, 622700, 622979, 623262, 623558, 623847, 624142, 624444, 624753, 625040, 625303, 625600, 625923, 626186, 626486, 626794, 627088, 627365, 627657, 627947, 628226, 628533, 628862, 629134, 629414, 629701, 629989, 630273, 630576, 630877, 631173, 631459, 631758, 632039, 632323, 632614, 632916, 633227, 633520, 633799, 634095, 634366, 634649, 634973, 635265, 635566, 635850, 636141, 636426, 636707, 637003, 637303, 637612, 637925, 638188, 638476, 638757, 639054, 639357, 639665, 639957, 640243, 640555, 640813, 641097, 641396, 641710, 642005, 642290, 642586, 642866, 643147, 643443, 643751, 644052, 644340, 644634, 644919, 645197, 645495, 645788, 646097, 646391, 646682, 646970, 647247, 647539, 647836, 648169, 648442, 648730, 649022, 649300, 649580, 649876]\n",
      "['/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', 'Q', 'Q', 'Q', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'f', 'N', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'N', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'Q', 'Q', 'f', 'f', 'f', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'V', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'f', 'f', 'N', 'f', 'f', 'N', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'Q', 'Q', 'Q', 'Q', 'f', 'f', 'f', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'f', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'f', 'N', 'N', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'f', 'N', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'f', 'N', 'f', 'f', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'f', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'N', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'V', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', 'f', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'N', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'N', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', 'f', '/', '/', '/', '/', '/', '/', 'f', 'f', '/', '/', '/', '/', '/', '/']\n"
     ]
    }
   ],
   "source": [
    "print(half_beat_locations[0])\n",
    "print(half_beat_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[324907, 325258, 325591, 325929, 326290, 326643, 326995, 327331, 327664, 328006, 328363, 328716, 329062, 329404, 329745, 330079, 330430, 330783, 331138, 331473, 331809, 332134, 332482, 332847, 333191, 333538, 333885, 334218, 334560, 334915, 335270, 335618, 335950, 336285, 336623, 336982, 337333, 337676, 338020, 338360, 338692, 339039, 339400, 339753, 340093, 340428, 340758, 341112, 341480, 341828, 342179, 342525, 342861, 343199, 343559, 343919, 344269, 344613, 344950, 345300, 345663, 346019, 346372, 346717, 347060, 347398, 347753, 348121, 348471, 348813, 349154, 349487, 349780, 350145, 350500, 350861, 351206, 351540, 351888, 352249, 352616, 352963, 353304, 353645, 353995, 354356, 354707, 355053, 355398, 355739, 356073, 356436, 356804, 357152, 357496, 357834, 358174, 358538, 358897, 359249, 359599, 359942, 360272, 360630, 360997, 361357, 361707, 362046, 362393, 362752, 363108, 363464, 363813, 364160, 364493, 364843, 365218, 365572, 365922, 366263, 366600, 366954, 367311, 367663, 368018, 368360, 368696, 369035, 369395, 369762, 370110, 370448, 370791, 371128, 371481, 371838, 372182, 372533, 372874, 373201, 373563, 373929, 374278, 374621, 374959, 375294, 375646, 376007, 376361, 376717, 377060, 377394, 377742, 378105, 378467, 378818, 379157, 379506, 379845, 380203, 380563, 380912, 381264, 381603, 381943, 382312, 382669, 383013, 383355, 383713, 384051, 384405, 384769, 385123, 385479, 385815, 386151, 386503, 386866, 387222, 387563, 387909, 388246, 388594, 388959, 389312, 389666, 390010, 390346, 390689, 391058, 391417, 391757, 392100, 392442, 392786, 393148, 393507, 393870, 394216, 394553, 394894, 395254, 395623, 395974, 396318, 396673, 397011, 397371, 397733, 398090, 398441, 398784, 399118, 399479, 399844, 400195, 400539, 400880, 401218, 401568, 401929, 402289, 402637, 402972, 403307, 403651, 404017, 404372, 404715, 405064, 405405, 405743, 406099, 406457, 406812, 407150, 407486, 407822, 408183, 408542, 408891, 409236, 409579, 409915, 410268, 410628, 410988, 411334, 411669, 412003, 412359, 412730, 413082, 413432, 413783, 414119, 414464, 414824, 415184, 415535, 415876, 416214, 416561, 416926, 417282, 417630, 417973, 418318, 418653, 419007, 419375, 419722, 420064, 420407, 420737, 421105, 421465, 421811, 422163, 422502, 422836, 423183, 423543, 423900, 424249, 424587, 424921, 425279, 425638, 425987, 426336, 426675, 427019, 427358, 427714, 428082, 428431, 428773, 429113, 429453, 429821, 430178, 430527, 430879, 431224, 431558, 431919, 432287, 432646, 432996, 433337, 433684, 434048, 434407, 434764, 435113, 435461, 435795, 436150, 436523, 436880, 437228, 437568, 437903, 438260, 438619, 438975, 439326, 439668, 440008, 440349, 440714, 441077, 441426, 441770, 442110, 442457, 442814, 443172, 443523, 443872, 444215, 444551, 444919, 445283, 445636, 445981, 446320, 446665, 447026, 447390, 447745, 448099, 448446, 448781, 449142, 449510, 449867, 450215, 450556, 450903, 451257, 451617, 451973, 452325, 452674, 453009, 453361, 453734, 454091, 454436, 454777, 455121, 455466, 455828, 456185, 456541, 456888, 457221, 457562, 457924, 458285, 458636, 458974, 459318, 459659, 460016, 460374, 460728, 461081, 461417, 461751, 462113, 462474, 462824, 463164, 463507, 463846, 464199, 464563, 464917, 465272, 465612, 465944, 466297, 466664, 467025, 467374, 467721, 468069, 468417, 468782, 469142, 469499, 469850, 470192, 470536, 470909, 471277, 471622, 471973, 472323, 472669, 473031, 473396, 473757, 474103, 474444, 474783, 475143, 475512, 475859, 476206, 476558, 476890, 477251, 477609, 477963, 478314, 478654, 478986, 479350, 479713, 480064, 480415, 480759, 481097, 481455, 481815, 482182, 482536, 482874, 483212, 483568, 483942, 484302, 484652, 485009, 485350, 485698, 486064, 486427, 486785, 487130, 487473, 487828, 488199, 488559, 488914, 489266, 489612, 489958, 490320, 490692, 491048, 491391, 491734, 492079, 492453, 492811, 493161, 493515, 493852, 494189, 494549, 494908, 495267, 495610, 495951, 496293, 496654, 497016, 497371, 497716, 498064, 498400, 498750, 499122, 499481, 499830, 500175, 500510, 500875, 501243, 501596, 501953, 502303, 502639, 502987, 503354, 503717, 504070, 504418, 504761, 505115, 505481, 505842, 506198, 506551, 506899, 507245, 507622, 507994, 508350, 508703, 509045, 509401, 509774, 510136, 510495, 510853, 511197, 511538, 511907, 512274, 512628, 512976, 513318, 513668, 514030, 514389, 514742, 515093, 515437, 515777, 516145, 516513, 516866, 517213, 517554, 517900, 518259, 518626, 518983, 519336, 519681, 520018, 520379, 520749, 521108, 521457, 521801, 522146, 522501, 522868, 523226, 523584, 523932, 524268, 524633, 525003, 525358, 525714, 526057, 526402, 526760, 527127, 527491, 527852, 528198, 528536, 528896, 529266, 529627, 529979, 530320, 530668, 531012, 531372, 531737, 532092, 532439, 532780, 533126, 533494, 533858, 534207, 534553, 534903, 535246, 535612, 535981, 536339, 536690, 537031, 537370, 537738, 538106, 538461, 538809, 539164, 539502, 539865, 540233, 540592, 540949, 541291, 541632, 541999, 542368, 542720, 543072, 543427, 543768, 544132, 544501, 544865, 545219, 545564, 545907, 546267, 546647, 547005, 547359, 547723, 548065, 548425, 548804, 549164, 549521, 549866, 550213, 550545, 550935, 551291, 551673, 551928, 552394, 552446, 552516, 552615, 553125, 553386, 553873, 554109, 554577, 554826, 554969, 555102, 555226, 555339, 555460, 555573, 555708, 555804, 555948, 556047, 556163, 556247, 556361, 556482, 556597, 556703, 556824, 556937, 557025, 557158, 557287, 557400, 557519, 557627, 557749, 557863, 557983, 558097, 558205, 558309, 558411, 558522, 558622, 558720, 558824, 558942, 559057, 559156, 559266, 559393, 559484, 559579, 559690, 559809, 559905, 560006, 560124, 560240, 560330, 560440, 560554, 560659, 560767, 560862, 560945, 561073, 561176, 561251, 561386, 561505, 561618, 561733, 561833, 561951, 562059, 562155, 562263, 562362, 562470, 562568, 562670, 562780, 562871, 562968, 563062, 563160, 563269, 563370, 563482, 563589, 563688, 563800, 563899, 564013, 564111, 564201, 564299, 564410, 564499, 564611, 564716, 564810, 564915, 565004, 565111, 565200, 565291, 565382, 565482, 565588, 565692, 565792, 565898, 565999, 566107, 566208, 566305, 566407, 566512, 566611, 566709, 566805, 566899, 566998, 567093, 567193, 567289, 567384, 567479, 567581, 567680, 567773, 567871, 567969, 568076, 568174, 568275, 568377, 568466, 568566, 568668, 568767, 568872, 568967, 569058, 569160, 569250, 569340, 569436, 569536, 569620, 569708, 569811, 569912, 570017, 570106, 570161, 570213, 570287, 570346, 570441, 570513, 570619, 570715, 570803, 570899, 570985, 571088, 571181, 571278, 571372, 571467, 571555, 571654, 571788, 571881, 571992, 572133, 572287, 572345, 572447, 572536, 572629, 572726, 572820, 572924, 573033, 573130, 573222, 573316, 573399, 573485, 573599, 573732, 573796, 573893, 573990, 574093, 574193, 574272, 574381, 574485, 574558, 574662, 574764, 574862, 574962, 575058, 575163, 575263, 575351, 575446, 575498, 575599, 575708, 575830, 575978, 576090, 576204, 576309, 576405, 576499, 576647, 576780, 576913, 577027, 577130, 577211, 577319, 577415, 577522, 577583, 577718, 577804, 577911, 577998, 578095, 578179, 578271, 578377, 578441, 578575, 578660, 578846, 578951, 579050, 579155, 579255, 579347, 579437, 579544, 579634, 579730, 579829, 579911, 580020, 580133, 580230, 580341, 580425, 580533, 580638, 580717, 580820, 580873, 580990, 581064, 581171, 581260, 581359, 581468, 581553, 581656, 581756, 581854, 581961, 582077, 582168, 582261, 582345, 582442, 582538, 582633, 582736, 582822, 582916, 583004, 583110, 583213, 583296, 583397, 583499, 583589, 583689, 583792, 583865, 583976, 584088, 584192, 584277, 584383, 584476, 584575, 584642, 584728, 584842, 584941, 585054, 585152, 585235, 585323, 585455, 585539, 585636, 585731, 585853, 586020, 586126, 586225, 586335, 586422, 586521, 586629, 586731, 586820, 586938, 587071, 587220, 587391, 587471, 587581, 587692, 587784, 587875, 587986, 588092, 588188, 588302, 588402, 588508, 588615, 588757, 588905, 589047, 589166, 589276, 589410, 589537, 589660, 590585, 591405, 592308, 592841, 593283, 593701, 594137, 594555, 594963, 595360, 595770, 596200, 596605, 597011, 597406, 597800, 598212, 598640, 599051, 599451, 599835, 600244, 600663, 601063, 601453, 601835, 602204, 602592, 602977, 603352, 603708, 604047, 604401, 604760, 605110, 605456, 605790, 606126, 606449, 606789, 607137, 607465, 607785, 608095, 608400, 608717, 609047, 609371, 609682, 609996, 610307, 610603, 610908, 611230, 611550, 611866, 612171, 612469, 613016, 613426, 613750, 614293, 614714, 615030, 615368, 615728, 616069, 616409, 616741, 617069, 617428, 617769, 618118, 618467, 618824, 619175, 619539, 619912, 620293, 620667, 621021, 621387, 621758, 622146, 622519, 622885, 623253, 623591, 623951, 624338, 624696, 625049, 625386, 625717, 626070, 626416, 626752, 627090, 627418, 627735, 628039, 628368, 628703, 629034, 629348, 629617, 629933, 630158, 630472, 630705, 630910, 631192, 631420, 631608, 631790, 631959, 632128, 632289, 632461, 632637, 632819, 632996, 633171, 633347, 633519, 633690, 633860, 634030, 634202, 634374, 634558, 634741, 634927, 635112, 635293, 635476, 635664, 635846, 636021, 636191, 636360, 636526, 636708, 636895, 637084, 637265, 637451, 637628, 637810, 637990, 638163, 638339, 638512, 638674, 638849, 639032, 639227, 639413, 639590, 639762, 639946, 640120, 640309, 640475, 640647, 640813, 640986, 641166, 641364, 641546, 641728, 641906, 642086, 642255, 642432, 642606, 642785, 642960, 643134, 643307, 643495, 643677, 643860, 644042, 644224, 644402, 644580, 644748, 644923, 645093, 645273, 645445, 645632, 645817, 646010, 646192, 646376, 646543, 646724, 646906, 647092, 647256, 647433, 647616, 647807, 647989, 648166, 648356, 648546, 648729, 648899, 649081, 649262, 649431, 649602, 649799]\n"
     ]
    }
   ],
   "source": [
    "print(test_beat_locations[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test beat locations, rhythm labels and locations\n",
    " #   with open('Test Data/Beat Locations.pkl', 'wb') as f:\n",
    "  #      pickle.dump(test_beat_locations,f)\n",
    "  #  with open('Test Data/Rhythm Locations.pkl', 'wb') as f:\n",
    "  #      pickle.dump(test_rhythm_locations,f)\n",
    "   # with open('Test Data/Rhythm Labels.pkl', 'wb') as f:\n",
    "  #      pickle.dump(test_rhythm_labels,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'a', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'a', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'a', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'j', 'j', 'N', 'j', 'N', 'N', 'N', 'a', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'V', 'N', 'a', 'N', 'a', 'N', 'N', 'N', 'V', 'N', 'N', 'a', 'N', 'N', 'x', 'N', 'x', 'N', 'x', 'N', 'N', 'a', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'x', 'V', 'N', 'N', 'x', 'N', 'N', 'N', 'N', 'a', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'V', 'a', 'a', 'N', 'a', 'a', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'a', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'a', 'a', 'N', 'N', 'N', 'N', 'N', 'a', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'j', 'N', 'j', 'A', 'j', 'A', 'N', 'N', 'V', 'J', 'N', 'x', 'N', 'x', 'N', 'x', 'N', 'x', 'N', 'x', 'N', 'x', 'N', 'x', 'N', 'x', 'N', 'x', 'N', 'x', 'N', 'N', 'V', 'N', 'N', 'x', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'a', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'V', 'N', 'a', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'a', 'N', 'N', 'V', 'N', 'a', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'a', 'N', 'N', 'N', 'V', 'N', 'a', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'a', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'N', 'a', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'a', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'a', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'N', 'A', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'a', 'a', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'A', 'N', 'N', 'A', 'N', 'a', 'j', 'N', 'N', 'N', 'N', 'A', 'A', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'a', 'j', 'A', 'N', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'V', 'A', 'A', 'N', 'N', 'V', 'A', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'A', 'A', 'N', 'N', 'V', 'N', 'a', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'A', 'A', 'N', 'N', 'N', 'N', 'V', 'N', 'a', 'a', 'N', 'N', 'V', 'N', 'N', 'a', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'N', 'a', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'a', 'a', 'N', 'N', 'N', 'V', 'A', 'A', 'N', 'V', 'N', 'N', 'V', 'N', 'a', 'N', 'N', 'V', 'N', 'N', 'V', 'A', 'N', 'N', 'N', 'V', 'A', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'a', 'a', 'N', 'N', 'a', 'N', 'N', 'V', 'N', 'N', 'N', 'a', 'N', 'N', 'V', 'N', 'N', 'N', 'V', 'N', 'a', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'x', 'N', 'N', 'N', 'V', 'N', 'a', 'N', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'V', 'a', 'A', 'N', 'N', 'a', 'N', 'N', 'a', 'a', 'N', 'N', 'F', 'N', 'a', 'N', 'a', 'N', 'x', 'N', 'a', 'N', 'a', 'N', 'x', 'N', 'a', 'N', 'a', 'N', 'a', 'N', 'x', 'N', 'a', 'N', 'a', 'N', 'a', 'A', 'N', 'a', 'N', 'x', 'N', 'a', 'N', 'a', 'a', 'N', 'a', 'N', 'x', 'N', 'a', 'N', 'a', 'N', 'a', 'N', 'N', 'A', 'N', 'a', 'N', 'x', 'N', 'a', 'N', 'A', 'N', 'a', 'N', 'x', 'N', 'a', 'a', 'N', 'a', 'a', 'N', 'x', 'N', 'N', 'a', 'N', 'N', 'a', 'N', 'x', 'x', 'N', 'a', 'N', 'N', 'N', 'V', 'N', 'N', 'N', 'a', 'N', 'N', 'a', 'A', 'N', 'N', 'N', 'A', 'A', 'N', 'A', 'A', 'N', 'A', 'N', 'x', 'N', 'a', 'a', 'N', 'a', 'a', 'N', 'N', 'a', 'N', 'a', 'N', 'x', 'F', 'a', 'N', 'x', 'N', 'a', 'N', 'x', 'N', 'x', 'N', 'a', 'N', 'x', 'N', 'a', 'a', 'A', 'N', 'x', 'N', 'N', 'a', 'N', 'x', 'N', 'a', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'a', 'N', 'N', 'a', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'a', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'j', 'N', 'j', 'N', 'N', 'a', 'N', 'x', 'N', 'N', 'a', 'N', 'x', 'N']\n"
     ]
    }
   ],
   "source": [
    "print(test_beat_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to remove the last beat from the arrays as this is usually erroneous due to disconnection of ECG\n",
    "# Can only do this for the latter half of each training set but for the first half of patients [28,40,41,44,45] we have only\n",
    "# The left hand side of the ECG so the last labels etc are fine. We appended these on last so the last 5 patients do not need\n",
    "# Changing\n",
    "\n",
    "for i in beat_locations:\n",
    "    del i[-1]\n",
    "for j in beat_labels:   \n",
    "    del j[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649441"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "beat_locations[6][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the starting and ending points of all the rhythms, we need to create windows of pre-determined\n",
    "# Sample size and then these will be given a target label in the form of a vector. This vector will tell us which \n",
    "# Rhythms are present within the sample.\n",
    "\n",
    "# Choose a sample size - The number of beat labels you are going to send in\n",
    "sample_size = 10\n",
    "\n",
    "# Now we need to create sublists for each patient with 25 beat labels in\n",
    "full_list = []\n",
    "for j in beat_labels:\n",
    "    samples = [j[i * sample_size:(i + 1) * sample_size] for i in range((len(j) + sample_size - 1) // sample_size )]\n",
    "    full_list.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n",
      "['N', 'N']\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(len(full_list[0]))\n",
    "print(full_list[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to replace each type with their corresponding integer number\n",
    "integer_samples = [[[0 if b == 'N'\n",
    "          else 1 if b == 'L'\n",
    "          else 2 if b == 'R'\n",
    "          else 3 if b == 'A'\n",
    "          else 4 if b == 'a'\n",
    "          else 5 if b == 'J'\n",
    "          else 6 if b == 'S'\n",
    "          else 7 if b == 'V'\n",
    "          else 8 if b == 'F'\n",
    "          else 9 if b == '!'\n",
    "          else 10 if b == 'e'\n",
    "          else 11 if b == 'j'\n",
    "          else 12 if b == 'E'\n",
    "          else 13 if b == '/'\n",
    "          else 14 if b == 'f'\n",
    "          else 15 if b == 'x'\n",
    "          else 16 if b == 'Q'\n",
    "          else 17 for b in j] for j in k] for k in full_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(integer_samples[0][90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/', 'f', 'f', '/', '/', '/']\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(full_list[2][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now some of the samples will be missing beats so for these we need to pad them with arbitrary values \n",
    "# To make them 25 in length\n",
    "\n",
    "# Loop over the samples and pad the last elements\n",
    "for i in integer_samples:\n",
    "    for c,j in enumerate(i):\n",
    "        if (c == (len(i) - 1)):\n",
    "            # Pad last sample\n",
    "            element_to_add = sample_size - len(j)\n",
    "            for k in range(element_to_add):\n",
    "                new = [99]\n",
    "                j = j + new\n",
    "            i[c] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 99, 99]\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(integer_samples[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have loads of samples in integer format with the encoding from above. We now need to go through and\n",
    "# Create new vector label arrays of dimension (15,) in form [1,0,0,0.....] if it is a Atrial bigeminy\n",
    "# [0,1,0,0,0,0...] if it is a Atrial fibrillation etc. If the sample contains a mixture of rhythms then we opt for\n",
    "# A label such as [1,1,0,0,0....] for AF and atrial bigeminy\n",
    "\n",
    "# Now we need to create sublists for each patient with 25 beat label locations in\n",
    "location_list = []\n",
    "for j in beat_locations:\n",
    "    samples = [j[i * sample_size:(i + 1) * sample_size] for i in range((len(j) + sample_size - 1) // sample_size )]\n",
    "    location_list.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 35142, 53625, 120082, 202337]\n",
      "[323717, 324161, 324418]\n",
      "['SBR']\n"
     ]
    }
   ],
   "source": [
    "print(rhythm_locations[-2])\n",
    "print(location_list[-1][-1])\n",
    "print(rhythm_labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "# Now we need to loop over every sample and check where the beat labels fit into with regards to the rhythm ranges\n",
    "# For ease we will assume the first few samples up the rhythm label are also the same rhythm.\n",
    "# Rhythm labels:\n",
    "# AB - atrial bigeminy (0), AFIB - atrial fibrillation(1), AFL - atrial flutter(2), B - ventricular bigeminy(3)\n",
    "# BII - 2 heart block(4), IVR - idioventricular rhythm(5), N - normal sinus rhythm(6), NOD - nodal rhythm(7)\n",
    "# P - paced rhythm(8), PREX - pre-excitation(9), SBR - sinus brachycardia(10), SVTA - supraventricular tachyarrhymia(11)\n",
    "# T - ventricular trigeminy(12), VFL - ventricular flutter(13), VT - ventricular tachycardia(14)\n",
    "\n",
    "sample_labels = []\n",
    "\n",
    "# First pick a patient\n",
    "for i,patient in enumerate(rhythm_locations):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    patient_beat_labels = []\n",
    "    \n",
    "    # Now loop over all the beat labels in that patients data\n",
    "    for beat_location in beat_locations[i]:\n",
    "        \n",
    "        # Loop over all the rhythms and find which one it is after and use that rhythm label\n",
    "        \n",
    "        rhythm_after = []\n",
    "        \n",
    "        for c,rhythm_location in reversed(list(enumerate(patient))):\n",
    "            if (len(rhythm_after) > 0):\n",
    "                break\n",
    "            else:\n",
    "                if(beat_location > rhythm_location):\n",
    "                    rhythm_after.append(rhythm_labels[i][c])\n",
    "                    #print(rhythm_labels[i][c])\n",
    "                \n",
    "        patient_beat_labels.append(rhythm_after)\n",
    "        #print(patient_beat_labels[-1])\n",
    "       # print(beat_location)\n",
    "        #print(i)\n",
    "        \n",
    "    sample_labels.append(patient_beat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR'], ['SBR']]\n",
      "2531\n",
      "[21425, 21668, 21910, 22160, 22408, 22661, 22922, 23189, 23439, 23677, 23918, 24158, 24408, 24664, 24923, 25180, 25448, 25699, 25938, 26178]\n",
      "[18]\n",
      "['N']\n"
     ]
    }
   ],
   "source": [
    "print(sample_labels[-1])\n",
    "print(len(beat_locations[8]))\n",
    "print(beat_locations[8][90:110])\n",
    "print(rhythm_locations[8])\n",
    "print(rhythm_labels[8][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the rhythms set up we need to segment into blocks of sample_size again\n",
    "\n",
    "full_list_labels = []\n",
    "for j in sample_labels:\n",
    "    samples_labels = [j[i * sample_size:(i + 1) * sample_size] for i in range((len(j) + sample_size - 1) // sample_size )]\n",
    "    full_list_labels.append(samples_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "# Per patient\n",
    "for c,i in enumerate(full_list_labels):\n",
    "    print(c)\n",
    "    patient = []\n",
    "    # Each sample\n",
    "    for j in i:\n",
    "        new = []\n",
    "        # Get rid of annoying list notation\n",
    "        for k in j:\n",
    "            try:\n",
    "                new.append(k[0])\n",
    "            except IndexError:\n",
    "                print(k)\n",
    "            #new.append(k[0])\n",
    "        patient.append(new)\n",
    "    x.append(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N']\n"
     ]
    }
   ],
   "source": [
    "print(x[-2][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(integer_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in an array of sample_labels as well as a boolean ~(tells us if there is a mixture or not)~\n",
    "# We then create a corresponding vector for that sample\n",
    "def create_complete_label(sample_labels):\n",
    "\n",
    "    # Doing this finds the UNIQUE elements of the sample_labels\n",
    "    unique_elements = list(set(sample_labels))\n",
    "    \n",
    "    #print(unique_elements)\n",
    "    #for i in unique_elements:\n",
    "        #print(i)\n",
    "        \n",
    "    # Create vector - some reason they have a bracket in front of all of them when you extract them\n",
    "    label = [0 if b == 'AB'\n",
    "        else 1 if b == 'AFIB'\n",
    "        else 2 if b == 'AFL'\n",
    "        else 3 if b == 'B'\n",
    "        else 4 if b == 'BII'\n",
    "        else 5 if b == 'IVR'\n",
    "        else 6 if b == 'N'\n",
    "        else 7 if b == 'NOD'\n",
    "        else 8 if b == 'P'\n",
    "        else 9 if b == 'PREX'\n",
    "        else 10 if b == 'SBR'\n",
    "        else 11 if b == 'SVTA'\n",
    "        else 12 if b == 'T'\n",
    "        else 13 if b == 'VFL'\n",
    "        # This one is VT label\n",
    "        else 14 for b in unique_elements]\n",
    "        \n",
    "    # Return the vector\n",
    "    return(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_labels = []\n",
    "for i in x:\n",
    "    patient_l = []\n",
    "    for j in i:\n",
    "        patient_l.append(create_complete_label(j))\n",
    "        \n",
    "    complete_labels.append(patient_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10], [10]]\n",
      "['N']\n"
     ]
    }
   ],
   "source": [
    "print(complete_labels[-1])\n",
    "print(rhythm_labels[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to transfer these into vectors like [1,0,0,0,0,0,0,0,0,0...] if label 1 is present or\n",
    "# [1,1,0,0,0,0,0,0,0....] if a mixture of label 1 or 2 are present etc.\n",
    "\n",
    "y = []\n",
    "\n",
    "# Loop over every sample and create a vector for it and append it to a list\n",
    "for c,patient in enumerate(complete_labels):\n",
    "    \n",
    "    for sample in patient:\n",
    "        \n",
    "        # Create a label vector\n",
    "        temp = np.zeros((15,))\n",
    "        \n",
    "        # Find which numbers are present, then these indices need to be set to 1\n",
    "        for item in sample:\n",
    "            temp[item] = 1\n",
    "            \n",
    "        y.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 3, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 3, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 3], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 3, 0], [0, 0, 0, 0, 0, 3, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 3, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 0, 0, 0, 0, 3, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 3], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 3, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 3, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 3], [0, 0, 3, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 0, 0, 0, 0, 0, 0, 0, 3, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 0, 0, 0, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 3, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 3, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 3, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 7, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 3, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 3, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 3, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 99, 99, 99, 99, 99, 99, 99, 99]]\n",
      "9986\n"
     ]
    }
   ],
   "source": [
    "# Have complete set of target labels and input arrays\n",
    "# Check\n",
    "print(integer_samples[0])\n",
    "# Need to convert integer_samples to a 1 dimensional array of 25 sample windows\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in integer_samples:\n",
    "    for j in i:\n",
    "        x.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 99, 99, 99])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9986, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape(len(y), sample_size,1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9986, 15)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y[170])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define a function that finds proportions of the different labels\n",
    "\n",
    "def proportions(labels):\n",
    "    \n",
    "    # Data is in vector form so [1,0,0,0,0....] etc with this indicating it is the first rhythm\n",
    "    # We need to loop over the vector to find where the 1's are then we can get the proportion that is a certain type\n",
    "    \n",
    "    n_mixed, n_pure, n_AB, n_AFIB, n_AFL, n_B, n_BII, n_IVR, n_N, n_NOD, n_P, n_PREX, n_SBR, n_SVTA, n_T, n_VFL, n_VT = 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "    \n",
    "    options = {0 : n_AB,\n",
    "           1 : n_AFIB,\n",
    "           2 : n_AFL,\n",
    "           3 : n_B,\n",
    "           4 : n_BII,\n",
    "           5 : n_IVR,\n",
    "           6 : n_N,\n",
    "           7 : n_NOD,\n",
    "           8 : n_P,\n",
    "           9 : n_PREX,\n",
    "          10 : n_SBR,\n",
    "          11 : n_SVTA,\n",
    "          12 : n_T,\n",
    "          13 : n_VFL,\n",
    "          14 : n_VT,\n",
    "          15 : n_mixed,\n",
    "          16 : n_pure\n",
    "}\n",
    "\n",
    "    for i in labels:\n",
    "        #print(i)\n",
    "        mix = False\n",
    "        count = 0\n",
    "        for index,number in enumerate(i):\n",
    "            #print(count)\n",
    "            #print(number)\n",
    "            \n",
    "            #if ((number == 1) and (index == 0)):\n",
    "                #print(i)\n",
    "            \n",
    "            if(number == 1):\n",
    "                count += 1\n",
    "            \n",
    "                # Change variable in dictionary to one up\n",
    "                options[index] += 1\n",
    "                \n",
    "            elif(number == 0):\n",
    "                count = count\n",
    "        \n",
    "        if(count >= 2):\n",
    "                mix = True\n",
    "                options[15] += 1\n",
    "        elif(count == 1):\n",
    "                mix = False\n",
    "                options[16] += 1\n",
    "                \n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = proportions(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5, 1: 1105, 2: 88, 3: 456, 4: 18, 5: 6, 6: 7660, 7: 21, 8: 739, 9: 138, 10: 92, 11: 48, 12: 176, 13: 89, 14: 77, 15: 708, 16: 9278}\n",
      "9986\n"
     ]
    }
   ],
   "source": [
    "print(data_split)\n",
    "\n",
    "# This checks we have correct number of samples\n",
    "print(data_split[15] + data_split[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training data and training labels\n",
    "#with open('RNN Training Data.pkl', 'wb') as f:\n",
    " #   pickle.dump(x, f)\n",
    "#with open('RNN Training Targets.pkl', 'wb') as f:\n",
    " #   pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold1\n",
      "Train set\n",
      "{0: 5, 1: 995, 2: 79, 3: 410, 4: 16, 5: 6, 6: 6894, 7: 19, 8: 665, 9: 125, 10: 83, 11: 43, 12: 159, 13: 80, 14: 70, 15: 634, 16: 8358}\n",
      "Validation set\n",
      "{0: 0, 1: 110, 2: 9, 3: 46, 4: 2, 5: 0, 6: 766, 7: 2, 8: 74, 9: 13, 10: 9, 11: 5, 12: 17, 13: 9, 14: 7, 15: 74, 16: 920}\n",
      "Fold2\n",
      "Train set\n",
      "{0: 5, 1: 994, 2: 80, 3: 410, 4: 16, 5: 6, 6: 6894, 7: 19, 8: 665, 9: 124, 10: 83, 11: 42, 12: 159, 13: 81, 14: 69, 15: 626, 16: 8373}\n",
      "Validation set\n",
      "{0: 0, 1: 111, 2: 8, 3: 46, 4: 2, 5: 0, 6: 766, 7: 2, 8: 74, 9: 14, 10: 9, 11: 6, 12: 17, 13: 8, 14: 8, 15: 82, 16: 905}\n",
      "Fold3\n",
      "Train set\n",
      "{0: 4, 1: 994, 2: 80, 3: 410, 4: 16, 5: 6, 6: 6894, 7: 19, 8: 665, 9: 125, 10: 83, 11: 44, 12: 158, 13: 80, 14: 69, 15: 643, 16: 8337}\n",
      "Validation set\n",
      "{0: 1, 1: 111, 2: 8, 3: 46, 4: 2, 5: 0, 6: 766, 7: 2, 8: 74, 9: 13, 10: 9, 11: 4, 12: 18, 13: 9, 14: 8, 15: 65, 16: 941}\n",
      "Fold4\n",
      "Train set\n",
      "{0: 4, 1: 995, 2: 79, 3: 411, 4: 16, 5: 6, 6: 6894, 7: 18, 8: 665, 9: 124, 10: 83, 11: 43, 12: 158, 13: 80, 14: 69, 15: 639, 16: 8343}\n",
      "Validation set\n",
      "{0: 1, 1: 110, 2: 9, 3: 45, 4: 2, 5: 0, 6: 766, 7: 3, 8: 74, 9: 14, 10: 9, 11: 5, 12: 18, 13: 9, 14: 8, 15: 69, 16: 935}\n",
      "Fold5\n",
      "Train set\n",
      "{0: 4, 1: 995, 2: 79, 3: 411, 4: 16, 5: 4, 6: 6894, 7: 19, 8: 665, 9: 124, 10: 83, 11: 43, 12: 159, 13: 80, 14: 69, 15: 634, 16: 8355}\n",
      "Validation set\n",
      "{0: 1, 1: 110, 2: 9, 3: 45, 4: 2, 5: 2, 6: 766, 7: 2, 8: 74, 9: 14, 10: 9, 11: 5, 12: 17, 13: 9, 14: 8, 15: 74, 16: 923}\n",
      "Fold6\n",
      "Train set\n",
      "{0: 5, 1: 995, 2: 79, 3: 410, 4: 16, 5: 6, 6: 6894, 7: 19, 8: 665, 9: 124, 10: 83, 11: 43, 12: 159, 13: 80, 14: 69, 15: 649, 16: 8328}\n",
      "Validation set\n",
      "{0: 0, 1: 110, 2: 9, 3: 46, 4: 2, 5: 0, 6: 766, 7: 2, 8: 74, 9: 14, 10: 9, 11: 5, 12: 17, 13: 9, 14: 8, 15: 59, 16: 950}\n",
      "Fold7\n",
      "Train set\n",
      "{0: 5, 1: 994, 2: 79, 3: 411, 4: 16, 5: 5, 6: 6894, 7: 19, 8: 665, 9: 124, 10: 83, 11: 43, 12: 158, 13: 80, 14: 70, 15: 646, 16: 8331}\n",
      "Validation set\n",
      "{0: 0, 1: 111, 2: 9, 3: 45, 4: 2, 5: 1, 6: 766, 7: 2, 8: 74, 9: 14, 10: 9, 11: 5, 12: 18, 13: 9, 14: 7, 15: 62, 16: 947}\n",
      "Fold8\n",
      "Train set\n",
      "{0: 4, 1: 995, 2: 79, 3: 410, 4: 16, 5: 5, 6: 6894, 7: 19, 8: 665, 9: 124, 10: 83, 11: 44, 12: 158, 13: 80, 14: 70, 15: 629, 16: 8371}\n",
      "Validation set\n",
      "{0: 1, 1: 110, 2: 9, 3: 46, 4: 2, 5: 1, 6: 766, 7: 2, 8: 74, 9: 14, 10: 9, 11: 4, 12: 18, 13: 9, 14: 7, 15: 79, 16: 907}\n",
      "Fold9\n",
      "Train set\n",
      "{0: 5, 1: 994, 2: 79, 3: 411, 4: 17, 5: 5, 6: 6894, 7: 19, 8: 665, 9: 124, 10: 82, 11: 43, 12: 158, 13: 80, 14: 69, 15: 634, 16: 8356}\n",
      "Validation set\n",
      "{0: 0, 1: 111, 2: 9, 3: 45, 4: 1, 5: 1, 6: 766, 7: 2, 8: 74, 9: 14, 10: 10, 11: 5, 12: 18, 13: 9, 14: 8, 15: 74, 16: 922}\n",
      "Fold10\n",
      "Train set\n",
      "{0: 4, 1: 994, 2: 79, 3: 410, 4: 17, 5: 5, 6: 6894, 7: 19, 8: 666, 9: 124, 10: 82, 11: 44, 12: 158, 13: 80, 14: 69, 15: 638, 16: 8350}\n",
      "Validation set\n",
      "{0: 1, 1: 111, 2: 9, 3: 46, 4: 1, 5: 1, 6: 766, 7: 2, 8: 73, 9: 14, 10: 10, 11: 4, 12: 18, 13: 9, 14: 8, 15: 70, 16: 928}\n"
     ]
    }
   ],
   "source": [
    "# As some labels are not present frequently in the sample, we need to implement a k fold validation scheme\n",
    "# In the hope that this will produce a more representative set.\n",
    "\n",
    "import math\n",
    "\n",
    "batch_size = 28\n",
    "\n",
    "# Many thanks to creators of iterative stratification and scikit-multilearn\n",
    "# Reference :\n",
    "# If you use this method to stratify data please cite both:\n",
    "# 1 -> Sechidis, K., Tsoumakas, G., & Vlahavas, I. (2011). On the stratification of multi-label data. Machine Learning and Knowledge Discovery in Databases, 145-158. http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf\n",
    "# 2 -> Piotr Szymański, Tomasz Kajdanowicz ; Proceedings of the First International Workshop on Learning with Imbalanced Domains: Theory and Applications, PMLR 74:22-35, 2017. http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html\n",
    "# Found on http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html\n",
    "\n",
    "\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    " \n",
    "# The more splits you do the more likely the data is representative however this comes with trade off that\n",
    "# You will have a lower percentage of data in your test set as well as the fact that it will take significantly\n",
    "# Longer computationally. Thankfully the RNN is very fast as we are just using sequences so a 10-fold validation\n",
    "# Will be sufficient and leaves 90% of the data for training purposes\n",
    "\n",
    "n_split = 10\n",
    " \n",
    "k_fold = IterativeStratification(n_splits = n_split, order=1)\n",
    "\n",
    "i = 1\n",
    "\n",
    "# This is just to check that splits are somewhat correct and then crucially saves the data set splits for each fold\n",
    "# Into a file for analysis later\n",
    "\n",
    "# Save the training and validation indices for each fold so that we can use them on the server\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for train, val in k_fold.split(x, y):\n",
    "    print(\"Fold\" + str(i))\n",
    "    temp_1 = proportions(y[train])\n",
    "    temp_2 = proportions(y[val])\n",
    "    print(\"Train set\")\n",
    "    print(temp_1)\n",
    "    print(\"Validation set\")\n",
    "    print(temp_2)\n",
    "    i += 1\n",
    "    train_indices.append(train)\n",
    "    val_indices.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save the indices for GPU use on server\n",
    "#with open('RNN Training Indices.pkl', 'wb') as f:\n",
    " #   pickle.dump(train_indices, f)\n",
    "#with open('RNN Validation Indices.pkl', 'wb') as f:\n",
    " #   pickle.dump(val_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "When passing a list as loss_weights, it should have one entry per model output. The model has 1 outputs, but you passed loss_weights=[998.6, 4.518552036199095, 56.73863636363637, 10.949561403508772, 277.3888888888889, 832.1666666666666, 0.6518276762402089, 237.76190476190476, 6.756427604871448, 36.18115942028985, 54.27173913043478, 104.02083333333333, 28.369318181818183, 56.10112359550562, 64.84415584415585]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-b506fda1c992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# Fit to data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;31m# Prepare list loss weights, same size of model outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         self.loss_weights_list = training_utils.prepare_loss_weights(\n\u001b[1;32m--> 143\u001b[1;33m             self.output_names, loss_weights)\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;31m# Prepare targets of model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mprepare_loss_weights\u001b[1;34m(output_names, loss_weights)\u001b[0m\n\u001b[0;32m    865\u001b[0m                              \u001b[1;34m'The model has '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                              \u001b[1;34m' outputs, but you passed loss_weights='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                              str(loss_weights))\n\u001b[0m\u001b[0;32m    868\u001b[0m         \u001b[0mweights_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: When passing a list as loss_weights, it should have one entry per model output. The model has 1 outputs, but you passed loss_weights=[998.6, 4.518552036199095, 56.73863636363637, 10.949561403508772, 277.3888888888889, 832.1666666666666, 0.6518276762402089, 237.76190476190476, 6.756427604871448, 36.18115942028985, 54.27173913043478, 104.02083333333333, 28.369318181818183, 56.10112359550562, 64.84415584415585]"
     ]
    }
   ],
   "source": [
    "# Data folds are ready to go so now we need to train on every fold and save a couple of things:\n",
    "# 1. Data set for each fold\n",
    "# 2. Actual data for each fold\n",
    "# 3. Accuracies on validation and training set for each fold\n",
    "# 4. Average accuracy and loss over 10 folds\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, multilabel_confusion_matrix, hamming_loss, jaccard_score\n",
    "\n",
    "i = 1\n",
    "\n",
    "# Need to save the scores for each fold evaluation\n",
    "scores = np.zeros((n_split, 4))\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "for train, val in k_fold.split(x, y):\n",
    "    \n",
    "    print(\"Fold \" + str(i))\n",
    "    \n",
    "    train_x = x[train]\n",
    "    train_y = y[train]\n",
    "    val_x = x[val]\n",
    "    val_y = y[val]\n",
    "    \n",
    "    # Find new proportions and save them\n",
    "    total = []\n",
    "    data_split_train = proportions(train_y)\n",
    "    total.append(data_split_train)\n",
    "    data_split_val = proportions(val_y)\n",
    "    total.append(data_split_val)\n",
    "    \n",
    "    with open('10-Fold Validation/Data Splits For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "        pickle.dump(total,f)\n",
    "    \n",
    "    #train_x,train_y = fit_to_batch(train_x,train_y,batch_size)\n",
    "    #val_x, val_y = fit_to_batch(val_x,val_y,batch_size)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units = 256,\n",
    "                            stateful = False,\n",
    "                            recurrent_dropout = 0.2,\n",
    "                            activation = 'sigmoid'),\n",
    "                            input_shape = (sample_size,1)))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(15, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy',loss_weights = list(weights), metrics=['accuracy'])\n",
    "    \n",
    "    # Fit to data\n",
    "    \n",
    "    history = model.fit(train_x, train_y, verbose = 1, batch_size = batch_size, validation_data = (val_x, val_y), epochs = 100, shuffle = True)\n",
    "    \n",
    "    # Save history for each fold\n",
    "    with open('10-Fold Validation/History For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "        pickle.dump(history,f)\n",
    "    \n",
    "    # Save the evaluate values for later\n",
    "    print(\"Evaluation on Training set\")\n",
    "    \n",
    "    train_scores = model.evaluate(train_x, train_y, batch_size = batch_size)\n",
    "    scores[i - 1][0] = train_scores[0]\n",
    "    scores[i - 1][1] = train_scores[1]\n",
    "    #print(train_scores)\n",
    "    \n",
    "    print(\"Evaluation on Validation set\")\n",
    "    \n",
    "    val_scores = model.evaluate(val_x, val_y, batch_size = batch_size)\n",
    "    scores[i - 1][2] = val_scores[0]\n",
    "    scores[i - 1][3] = val_scores[1]\n",
    "    predictions = model.predict(val_x)\n",
    "    # Turn to prediction multilabel outputs\n",
    "    predictions[predictions>=0.5] = 1\n",
    "    predictions[predictions<0.5] = 0\n",
    "    #print(predictions[0])\n",
    "    # Add in hamming loss, jaccard score, recall, precision, f1 score, then save multilabel confusion matrix\n",
    "    scikit_scores = []\n",
    "    scikit_scores.append(hamming_loss(val_y, predictions))\n",
    "    scikit_scores.append(jaccard_score(val_y, predictions, average = None))\n",
    "    scikit_scores.append(recall_score(val_y, predictions, average = None))\n",
    "    scikit_scores.append(precision_score(val_y, predictions, average = None))\n",
    "    scikit_scores.append(f1_score(val_y, predictions, average = None))\n",
    "    confusion = multilabel_confusion_matrix(val_y, predictions)\n",
    "    print(confusion)\n",
    "    #print(val_scores)\n",
    "    #print(scikit_scores)\n",
    "    #print(confusion)\n",
    "    \n",
    "    # Save the scikit parameters for this fold\n",
    "    with open('10-Fold Validation/Scikit Scores For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "        pickle.dump(scikit_scores,f)\n",
    "    \n",
    "    # Save the confusion matrix for this fold\n",
    "    with open('10-Fold Validation/Confusion Matrix For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "        pickle.dump(confusion,f)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "np.save('10-Fold Validation/Scores_Weighted.npy', scores, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (scores[0][1] + scores[1][1] + scores[2][1]) / 3\n",
    "print(confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
