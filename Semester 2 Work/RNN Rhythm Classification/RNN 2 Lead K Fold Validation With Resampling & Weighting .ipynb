{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Code Outline -------- #\n",
    "# Create a model that implements an RNN using output predictions from a CNN in order to identify rhythm changes\n",
    "# In this code we implement an over and under sampling scheme to try and improve validation accuracies as well as\n",
    "# A weighted binary crossentropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adb de-noised/100_de-noised.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "end = len(y)\n",
    "#print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will have time series data of labels for each patient. These series will be different lengths so we need to\n",
    "# Pad them all to add 0's to the end of each sequence to get longer length inputs. So first we save how many labels are\n",
    "# In each patient as well as how many beats are in the signal.\n",
    "\n",
    "# Load in the beat labels for each patient as well as the rhythm labels for each\n",
    "\n",
    "patient_id = range(0,48)\n",
    "\n",
    "beat_labels = []\n",
    "\n",
    "beat_locations = []\n",
    "\n",
    "rhythm_labels = []\n",
    "\n",
    "rhythm_locations = []\n",
    "\n",
    "full_test = [14,24]\n",
    "half_test = [4,28,40,41,44,45]\n",
    "\n",
    "test_beat_labels = []\n",
    "test_beat_locations = []\n",
    "test_rhythm_labels = []\n",
    "test_rhythm_locations = []\n",
    "\n",
    "half_beat_labels = []\n",
    "half_beat_locations = []\n",
    "half_rhythm_labels = []\n",
    "half_rhythm_locations = []\n",
    "\n",
    "for i in patient_id:\n",
    "    # Beat labels for patient i\n",
    "    with open('adb final labels/adb beat labels/{}_beat_labels.pkl'.format(i), 'rb') as f:\n",
    "        temp_beat = pickle.load(f)\n",
    "    # beat label locations for patient i\n",
    "    with open('adb final labels/adb peaks/{}_peaks.pkl'.format(i), 'rb') as f:\n",
    "        temp_beat_locations = pickle.load(f)\n",
    "    # rhythm labels for patient i\n",
    "    with open('adb final labels/adb rhythm labels/{}_rhythm_labels.pkl'.format(i), 'rb') as f:\n",
    "        temp_rhythm = pickle.load(f)\n",
    "    # Rhythm label locations for patient i\n",
    "    with open('adb final labels/adb rhythm locations/{}_rhythm_locations.pkl'.format(i), 'rb') as f:\n",
    "        temp_rhythm_location = pickle.load(f)\n",
    "        \n",
    "    # Separate the test and training sets\n",
    "    if(i in full_test):\n",
    "        test_rhythm_locations.append(temp_rhythm_location)\n",
    "        test_beat_labels.append(temp_beat)\n",
    "        test_rhythm_labels.append(temp_rhythm)\n",
    "        test_beat_locations.append(temp_beat_locations)\n",
    "    elif(i in half_test):\n",
    "        half_rhythm_locations.append(temp_rhythm_location)\n",
    "        half_beat_labels.append(temp_beat)\n",
    "        half_rhythm_labels.append(temp_rhythm)\n",
    "        half_beat_locations.append(temp_beat_locations)\n",
    "    else:\n",
    "        rhythm_locations.append(temp_rhythm_location)\n",
    "        beat_labels.append(temp_beat)\n",
    "        rhythm_labels.append(temp_rhythm)\n",
    "        beat_locations.append(temp_beat_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(rhythm_labels))\n",
    "#print(half_rhythm_labels[-1])\n",
    "#print(len(test_rhythm_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split the half patient test data into half in order to get representative test set\n",
    "index = []\n",
    "for j in half_beat_locations:\n",
    "    # Find beat location nearest to the halfway point\n",
    "    index.append(min(enumerate(j), key=lambda x: abs(x[1]-(end/2)))[0])\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the rhythm location closest to a certain beat location\n",
    "\n",
    "def find_beat_to_rhythm(beat_location, rhythm_location):\n",
    "    index, value = min(enumerate(rhythm_location), key=lambda x: abs(x[1]-(beat_location)))\n",
    "    return index, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have found the beat corresponding to the halfway point of the ecg, and then the function above finds the last rhythm\n",
    "# Label before this point\n",
    "\n",
    "# For every halfway beat in the list above we now need to find the nearest rhythm to it and use that as our starting rhythm\n",
    "\n",
    "for c,k in enumerate(index):\n",
    "    starting_rhythm_index, starting_rhythm_value = find_beat_to_rhythm(half_beat_locations[c][k], half_rhythm_locations[c])\n",
    "    halfway_list_locations = [end / 2]\n",
    "    halfway_list_labels = [half_rhythm_labels[c][starting_rhythm_index]]\n",
    "    \n",
    "    if(c == 0):\n",
    "        # We want left half of patient ECG in the test set\n",
    "        test_rhythm_labels.append(half_rhythm_labels[c][:starting_rhythm_index])\n",
    "        temp_list = half_rhythm_locations[c][:starting_rhythm_index] + [end / 2]\n",
    "        test_rhythm_locations.append(temp_list)\n",
    "        test_beat_labels.append(half_beat_labels[c][:k])\n",
    "        test_beat_locations.append(half_beat_locations[c][:k])\n",
    "        # Save the right half into the training set\n",
    "        temp_labels = halfway_list_labels + half_rhythm_labels[c][(starting_rhythm_index):]\n",
    "        rhythm_labels.append(temp_labels)\n",
    "        temp_locations = halfway_list_locations + half_rhythm_locations[c][(starting_rhythm_index):]\n",
    "        rhythm_locations.append(temp_locations)\n",
    "        beat_labels.append(half_beat_labels[c][k:])\n",
    "        beat_locations.append(half_beat_locations[c][k:])\n",
    "    else:\n",
    "        # We want right half of patient ECG in the test set\n",
    "        test_rhythm_labels.append(half_rhythm_labels[c][(starting_rhythm_index - 1):])\n",
    "        temp_list =  [end / 2] + half_rhythm_locations[c][starting_rhythm_index:]\n",
    "        test_rhythm_locations.append(temp_list)\n",
    "        test_beat_labels.append(half_beat_labels[c][k:])\n",
    "        test_beat_locations.append(half_beat_locations[c][k:])\n",
    "        # Save the left half into the training set\n",
    "        if(len(half_rhythm_labels[c]) == 1): # If we only have one rhythm present in the sample\n",
    "            temp_labels = [half_rhythm_labels[c][0]]\n",
    "            temp_locations = [half_rhythm_locations[c][0]]\n",
    "        else:\n",
    "            temp_labels = half_rhythm_labels[c][:starting_rhythm_index]\n",
    "            temp_locations = half_rhythm_locations[c][:starting_rhythm_index]\n",
    "        rhythm_labels.append(temp_labels)\n",
    "        rhythm_locations.append(temp_locations)\n",
    "        beat_labels.append(half_beat_labels[c][:k])\n",
    "        beat_locations.append(half_beat_locations[c][:k])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(rhythm_locations))\n",
    "#print(len(test_rhythm_locations))\n",
    "#print(test_rhythm_labels[2])\n",
    "#print(test_rhythm_locations[2])\n",
    "#print(rhythm_locations[-1])\n",
    "#print(rhythm_labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_beat_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to remove the last beat from the arrays as this is usually erroneous due to disconnection of ECG\n",
    "# Can only do this for the latter half of each training set but for the first half of patients [28,40,41,44,45] we have only\n",
    "# The left hand side of the ECG so the last labels etc are fine. We appended these on last so the last 5 patients do not need\n",
    "# Changing\n",
    "\n",
    "for i in beat_locations:\n",
    "    del i[-1]\n",
    "for j in beat_labels:   \n",
    "    del j[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#beat_locations[6][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the starting and ending points of all the rhythms, we need to create windows of pre-determined\n",
    "# Sample size and then these will be given a target label in the form of a vector. This vector will tell us which \n",
    "# Rhythms are present within the sample.\n",
    "\n",
    "# Choose a sample size - The number of beat labels you are going to send in\n",
    "sample_size = 10\n",
    "\n",
    "# Now we need to create sublists for each patient with 10 beat labels in\n",
    "full_list = []\n",
    "for j in beat_labels:\n",
    "    samples = [j[i * sample_size:(i + 1) * sample_size] for i in range((len(j) + sample_size - 1) // sample_size )]\n",
    "    full_list.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#print(len(full_list[0]))\n",
    "#print(full_list[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to replace each type with their corresponding integer number\n",
    "integer_samples = [[[0 if b == 'N'\n",
    "          else 1 if b == 'L'\n",
    "          else 2 if b == 'R'\n",
    "          else 3 if b == 'A'\n",
    "          else 4 if b == 'a'\n",
    "          else 5 if b == 'J'\n",
    "          else 6 if b == 'S'\n",
    "          else 7 if b == 'V'\n",
    "          else 8 if b == 'F'\n",
    "          else 9 if b == '!'\n",
    "          else 10 if b == 'e'\n",
    "          else 11 if b == 'j'\n",
    "          else 12 if b == 'E'\n",
    "          else 13 if b == '/'\n",
    "          else 14 if b == 'f'\n",
    "          else 15 if b == 'x'\n",
    "          else 16 if b == 'Q'\n",
    "          else 17 for b in j] for j in k] for k in full_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#print(integer_samples[0][90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#print(full_list[2][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now some of the samples will be missing beats so for these we need to pad them with arbitrary values \n",
    "# To make them 10 in length\n",
    "\n",
    "# Loop over the samples and pad the last elements\n",
    "for i in integer_samples:\n",
    "    for c,j in enumerate(i):\n",
    "        if (c == (len(i) - 1)):\n",
    "            # Pad last sample\n",
    "            element_to_add = sample_size - len(j)\n",
    "            for k in range(element_to_add):\n",
    "                new = [99]\n",
    "                j = j + new\n",
    "            i[c] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#print(integer_samples[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have loads of samples in integer format with the encoding from above. We now need to go through and\n",
    "# Create new vector label arrays of dimension (15,) in form [1,0,0,0.....] if it is a Atrial bigeminy\n",
    "# [0,1,0,0,0,0...] if it is a Atrial fibrillation etc. If the sample contains a mixture of rhythms then we opt for\n",
    "# A label such as [1,1,0,0,0....] for AF and atrial bigeminy\n",
    "\n",
    "# Now we need to create sublists for each patient with 25 beat label locations in\n",
    "location_list = []\n",
    "for j in beat_locations:\n",
    "    samples = [j[i * sample_size:(i + 1) * sample_size] for i in range((len(j) + sample_size - 1) // sample_size )]\n",
    "    location_list.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rhythm_locations[-2])\n",
    "#print(location_list[-1][-1])\n",
    "#print(rhythm_labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to loop over every sample and check where the beat labels fit into with regards to the rhythm ranges\n",
    "# For ease we will assume the first few samples up the rhythm label are also the same rhythm.\n",
    "# Rhythm labels:\n",
    "# AB - atrial bigeminy (0), AFIB - atrial fibrillation(1), AFL - atrial flutter(2), B - ventricular bigeminy(3)\n",
    "# BII - 2 heart block(4), IVR - idioventricular rhythm(5), N - normal sinus rhythm(6), NOD - nodal rhythm(7)\n",
    "# P - paced rhythm(8), PREX - pre-excitation(9), SBR - sinus brachycardia(10), SVTA - supraventricular tachyarrhymia(11)\n",
    "# T - ventricular trigeminy(12), VFL - ventricular flutter(13), VT - ventricular tachycardia(14)\n",
    "\n",
    "sample_labels = []\n",
    "\n",
    "# First pick a patient\n",
    "for i,patient in enumerate(rhythm_locations):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    patient_beat_labels = []\n",
    "    \n",
    "    # Now loop over all the beat labels in that patients data\n",
    "    for beat_location in beat_locations[i]:\n",
    "        \n",
    "        # Loop over all the rhythms and find which one it is after and use that rhythm label\n",
    "        \n",
    "        rhythm_after = []\n",
    "        \n",
    "        for c,rhythm_location in reversed(list(enumerate(patient))):\n",
    "            if (len(rhythm_after) > 0):\n",
    "                break\n",
    "            else:\n",
    "                if(beat_location > rhythm_location):\n",
    "                    rhythm_after.append(rhythm_labels[i][c])\n",
    "                    #print(rhythm_labels[i][c])\n",
    "                \n",
    "        patient_beat_labels.append(rhythm_after)\n",
    "        #print(patient_beat_labels[-1])\n",
    "       # print(beat_location)\n",
    "        #print(i)\n",
    "        \n",
    "    sample_labels.append(patient_beat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sample_labels[-1])\n",
    "#print(len(beat_locations[8]))\n",
    "#print(beat_locations[8][90:110])\n",
    "#print(rhythm_locations[8])\n",
    "#print(rhythm_labels[8][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the rhythms set up we need to segment into blocks of sample_size again\n",
    "\n",
    "full_list_labels = []\n",
    "for j in sample_labels:\n",
    "    samples_labels = [j[i * sample_size:(i + 1) * sample_size] for i in range((len(j) + sample_size - 1) // sample_size )]\n",
    "    full_list_labels.append(samples_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "# Per patient\n",
    "for c,i in enumerate(full_list_labels):\n",
    "    print(c)\n",
    "    patient = []\n",
    "    # Each sample\n",
    "    for j in i:\n",
    "        new = []\n",
    "        # Get rid of annoying list notation\n",
    "        for k in j:\n",
    "            try:\n",
    "                new.append(k[0])\n",
    "            except IndexError:\n",
    "                print(k)\n",
    "            #new.append(k[0])\n",
    "        patient.append(new)\n",
    "    x.append(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x[-2][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(integer_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in an array of sample_labels as well as a boolean ~(tells us if there is a mixture or not)~\n",
    "# We then create a corresponding vector for that sample\n",
    "def create_complete_label(sample_labels):\n",
    "\n",
    "    # Doing this finds the UNIQUE elements of the sample_labels\n",
    "    unique_elements = list(set(sample_labels))\n",
    "    \n",
    "    #print(unique_elements)\n",
    "    #for i in unique_elements:\n",
    "        #print(i)\n",
    "        \n",
    "    # Create vector - some reason they have a bracket in front of all of them when you extract them\n",
    "    label = [0 if b == 'AB'\n",
    "        else 1 if b == 'AFIB'\n",
    "        else 2 if b == 'AFL'\n",
    "        else 3 if b == 'B'\n",
    "        else 4 if b == 'BII'\n",
    "        else 5 if b == 'IVR'\n",
    "        else 6 if b == 'N'\n",
    "        else 7 if b == 'NOD'\n",
    "        else 8 if b == 'P'\n",
    "        else 9 if b == 'PREX'\n",
    "        else 10 if b == 'SBR'\n",
    "        else 11 if b == 'SVTA'\n",
    "        else 12 if b == 'T'\n",
    "        else 13 if b == 'VFL'\n",
    "        # This one is VT label\n",
    "        else 14 for b in unique_elements]\n",
    "        \n",
    "    # Return the vector\n",
    "    return(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_labels = []\n",
    "for i in x:\n",
    "    patient_l = []\n",
    "    for j in i:\n",
    "        patient_l.append(create_complete_label(j))\n",
    "        \n",
    "    complete_labels.append(patient_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(complete_labels[-1])\n",
    "#print(rhythm_labels[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to transfer these into vectors like [1,0,0,0,0,0,0,0,0,0...] if label 1 is present or\n",
    "# [1,1,0,0,0,0,0,0,0....] if a mixture of label 1 or 2 are present etc.\n",
    "\n",
    "y = []\n",
    "\n",
    "# Loop over every sample and create a vector for it and append it to a list\n",
    "for c,patient in enumerate(complete_labels):\n",
    "    \n",
    "    for sample in patient:\n",
    "        \n",
    "        # Create a label vector\n",
    "        temp = np.zeros((15,))\n",
    "        \n",
    "        # Find which numbers are present, then these indices need to be set to 1\n",
    "        for item in sample:\n",
    "            temp[item] = 1\n",
    "            \n",
    "        y.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have complete set of target labels and input arrays\n",
    "# Check\n",
    "#print(integer_samples[0])\n",
    "# Need to convert integer_samples to a 1 dimensional array of 10 sample windows\n",
    "#print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in integer_samples:\n",
    "    for j in i:\n",
    "        x.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(len(y), sample_size,1)\n",
    "#print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "#print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y[170])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define a function that finds proportions of the different labels\n",
    "\n",
    "def proportions(labels):\n",
    "    \n",
    "    # Data is in vector form so [1,0,0,0,0....] etc with this indicating it is the first rhythm\n",
    "    # We need to loop over the vector to find where the 1's are then we can get the proportion that is a certain type\n",
    "    \n",
    "    n_mixed, n_pure, n_AB, n_AFIB, n_AFL, n_B, n_BII, n_IVR, n_N, n_NOD, n_P, n_PREX, n_SBR, n_SVTA, n_T, n_VFL, n_VT = 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "    \n",
    "    options = {0 : n_AB,\n",
    "           1 : n_AFIB,\n",
    "           2 : n_AFL,\n",
    "           3 : n_B,\n",
    "           4 : n_BII,\n",
    "           5 : n_IVR,\n",
    "           6 : n_N,\n",
    "           7 : n_NOD,\n",
    "           8 : n_P,\n",
    "           9 : n_PREX,\n",
    "          10 : n_SBR,\n",
    "          11 : n_SVTA,\n",
    "          12 : n_T,\n",
    "          13 : n_VFL,\n",
    "          14 : n_VT,\n",
    "          15 : n_mixed,\n",
    "          16 : n_pure\n",
    "}\n",
    "\n",
    "    for i in labels:\n",
    "        #print(i)\n",
    "        mix = False\n",
    "        count = 0\n",
    "        for index,number in enumerate(i):\n",
    "            #print(count)\n",
    "            #print(number)\n",
    "            \n",
    "            #if ((number == 1) and (index == 0)):\n",
    "                #print(i)\n",
    "            \n",
    "            if(number == 1):\n",
    "                count += 1\n",
    "            \n",
    "                # Change variable in dictionary to one up\n",
    "                options[index] += 1\n",
    "                \n",
    "            elif(number == 0):\n",
    "                count = count\n",
    "        \n",
    "        if(count >= 2):\n",
    "                mix = True\n",
    "                options[15] += 1\n",
    "        elif(count == 1):\n",
    "                mix = False\n",
    "                options[16] += 1\n",
    "                \n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = proportions(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_split)\n",
    "\n",
    "# This checks we have correct number of samples\n",
    "#print(data_split[15] + data_split[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training data and training labels\n",
    "#with open('RNN Training Data.pkl', 'wb') as f:\n",
    " #   pickle.dump(x, f)\n",
    "#with open('RNN Training Targets.pkl', 'wb') as f:\n",
    " #   pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As some labels are not present frequently in the sample, we need to implement a k fold validation scheme\n",
    "# In the hope that this will produce a more representative set.\n",
    "\n",
    "import math\n",
    "\n",
    "batch_size = 28\n",
    "\n",
    "# Many thanks to creators of iterative stratification and scikit-multilearn\n",
    "# Reference :\n",
    "# If you use this method to stratify data please cite both:\n",
    "# 1 -> Sechidis, K., Tsoumakas, G., & Vlahavas, I. (2011). On the stratification of multi-label data. Machine Learning and Knowledge Discovery in Databases, 145-158. http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf\n",
    "# 2 -> Piotr Szymański, Tomasz Kajdanowicz ; Proceedings of the First International Workshop on Learning with Imbalanced Domains: Theory and Applications, PMLR 74:22-35, 2017. http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html\n",
    "# Found on http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html\n",
    "\n",
    "\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    " \n",
    "# The more splits you do the more likely the data is representative however this comes with trade off that\n",
    "# You will have a lower percentage of data in your test set as well as the fact that it will take significantly\n",
    "# Longer computationally. Thankfully the RNN is very fast as we are just using sequences so a 10-fold validation\n",
    "# Will be sufficient and leaves 90% of the data for training purposes\n",
    "\n",
    "n_split = 10\n",
    " \n",
    "k_fold = IterativeStratification(n_splits = n_split, order=1)\n",
    "\n",
    "i = 1\n",
    "\n",
    "# This is just to check that splits are somewhat correct and then crucially saves the data set splits for each fold\n",
    "# Into a file for analysis later\n",
    "\n",
    "# Save the training and validation indices for each fold so that we can use them on the server\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for train, val in k_fold.split(x, y):\n",
    "    print(\"Fold\" + str(i))\n",
    "    temp_1 = proportions(y[train])\n",
    "    temp_2 = proportions(y[val])\n",
    "    print(\"Train set\")\n",
    "    print(temp_1)\n",
    "    print(\"Validation set\")\n",
    "    print(temp_2)\n",
    "    i += 1\n",
    "    train_indices.append(train)\n",
    "    val_indices.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x.shape)\n",
    "#print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows us to downsample certain labels in this case normal beats\n",
    "\n",
    "def downsample(x,y,N,proportions):\n",
    "    \n",
    "    # Take out however many samples from normal beats\n",
    "    x = list(x)\n",
    "    y = list(y)\n",
    "\n",
    "    for i in range(N):\n",
    "        \n",
    "        for c,item in enumerate(y):\n",
    "            \n",
    "            if(item[6] == 1):\n",
    "                del y[c]\n",
    "                del x[c]\n",
    "                break;\n",
    "        \n",
    "    \n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that downsamples and upsamples via copying\n",
    "\n",
    "def sampler(x,y,N, proportions):\n",
    "\n",
    "    # Now they are split into training and validation sets, we need to keep the validation set as it is but\n",
    "    # Adjust the training set via resampling to get an even distribution of all the types of rhythms. There will be\n",
    "    # An optimal amount to do this, a little will increase accuracy but too much and we will overfit severely therefore\n",
    "    # Need to test a lot of different variations\n",
    "\n",
    "    # Looking at the proportions of the labels the lowest amount in any one class is 4 rhythms so first option to try is a sample\n",
    "    # Space where each corresponding rhythm only has ~ 5 samples in each, I do not imagine this will work well due to the low\n",
    "    # Number so it will not see all the different beat combinations for that type. It does however give us a base performance\n",
    "\n",
    "    # Easiest way to do this is to likely go through and pick randomly N from each label type ie [1,0,0...] or [0,1,0,...]\n",
    "    # And ignore the fact that some can be multilabel, this way we will get a split of multilabel or not and we will get slightly\n",
    "    # more types than others but not by big factors\n",
    "\n",
    "    sampled_x = []\n",
    "    sampled_y = []\n",
    "\n",
    "    # Loop over x and select 5 of each type giving 75 in total\n",
    "\n",
    "    for i in range(15):\n",
    "        \n",
    "        if(proportions(y)[i] < N):\n",
    "            # Check how many times we need to loop over the set to do copies\n",
    "            # This tells us per one we find in x, how many copies do we need to make of it\n",
    "            loops_to_copy = int(np.ceil(N/proportions(y)[i]))\n",
    "        else:\n",
    "            # Dont need to make a copy so just run through once\n",
    "            loops_to_copy = 1\n",
    "        \n",
    "        count = 0\n",
    "        for c,j in enumerate(y):\n",
    "\n",
    "            if(count == N):\n",
    "                #reset the count\n",
    "                count = 0\n",
    "                break\n",
    "\n",
    "            if(j[i] == 1):\n",
    "                \n",
    "                # If we need to make copies then do them here\n",
    "                for copy in range(loops_to_copy):\n",
    "                    sampled_y.append(j)\n",
    "                    sampled_x.append(x[i])\n",
    "                    count += 1\n",
    "                \n",
    "    return np.array(sampled_x), np.array(sampled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sampled_y[:2])\n",
    "#print(sampled_x[:2])\n",
    "#print(proportions(sampled_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save the indices for GPU use on server\n",
    "#with open('RNN Training Indices.pkl', 'wb') as f:\n",
    " #   pickle.dump(train_indices, f)\n",
    "#with open('RNN Validation Indices.pkl', 'wb') as f:\n",
    " #   pickle.dump(val_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a weighted loss that will account for the multilabel data being imbalanced\n",
    "# First find the appropriate weighting we need to give to each class\n",
    "def calculating_class_weights(y_true):\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    number_dim = np.shape(y_true)[1]\n",
    "    weights = np.empty([number_dim, 2])\n",
    "    for i in range(number_dim):\n",
    "        weights[i] = compute_class_weight('balanced', [0.,1.], y_true[:, i])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = calculating_class_weights(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a loss function that uses these new weights\n",
    "\n",
    "import keras.backend as K\n",
    "def get_weighted_loss(weights):\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data folds are ready to go so now we need to train on every fold and save a couple of things:\n",
    "# 1. Data set for each fold\n",
    "# 2. Actual data for each fold\n",
    "# 3. Accuracies on validation and training set for each fold\n",
    "# 4. Average accuracy and loss over 10 folds\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, multilabel_confusion_matrix, hamming_loss, jaccard_score\n",
    "from tensorflow.nn import sigmoid_cross_entropy_with_logits\n",
    "\n",
    "i = 1\n",
    "\n",
    "# Need to save the scores for each fold evaluation\n",
    "scores = np.zeros((n_split, 4))\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "for train, val in k_fold.split(x, y):\n",
    "    \n",
    "    print(\"Fold \" + str(i))\n",
    "    \n",
    "    train_x = x[train]\n",
    "    train_y = y[train]\n",
    "    \n",
    "    # Choose which resampling technique to use\n",
    "    sampled_x,sampled_y = downsample(train_x, train_y, 6000, proportions)\n",
    "    \n",
    "    sampled_x, sampled_y = sampler(train_x, train_y, 500, proportions)\n",
    "    \n",
    "    print(sampled_x.shape)\n",
    "    print(sampled_y.shape)\n",
    "    \n",
    "    class_weights = calculating_class_weights(sampled_y)\n",
    "    \n",
    "    val_x = x[val]\n",
    "    val_y = y[val]\n",
    "    \n",
    "    # Find new proportions and save them\n",
    "    total = []\n",
    "    data_split_train = proportions(train_y)\n",
    "    total.append(data_split_train)\n",
    "    data_split_val = proportions(val_y)\n",
    "    total.append(data_split_val)\n",
    "    \n",
    "   # with open('10-Fold Validation/Data Splits For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "    #    pickle.dump(total,f)\n",
    "    \n",
    "    #train_x,train_y = fit_to_batch(train_x,train_y,batch_size)\n",
    "    #val_x, val_y = fit_to_batch(val_x,val_y,batch_size)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units = 256,\n",
    "                            stateful = False,\n",
    "                            recurrent_dropout = 0.2,\n",
    "                            activation = 'sigmoid'),\n",
    "                            input_shape = (sample_size,1)))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(15, activation = 'sigmoid'))\n",
    "    \n",
    "    # Can change the loss here if we want to try weighted loss function\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Fit to data\n",
    "    \n",
    "    history = model.fit(sampled_x, sampled_y, verbose = 1, batch_size = batch_size, validation_data = (val_x, val_y), epochs = 50, shuffle = True)\n",
    "    \n",
    "    # Save history for each fold\n",
    "   # with open('10-Fold Validation/History For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "      #  pickle.dump(history,f)\n",
    "    \n",
    "    # Save the evaluate values for later\n",
    "    print(\"Evaluation on Training set\")\n",
    "    \n",
    "    train_scores = model.evaluate(train_x, train_y, batch_size = batch_size)\n",
    "    scores[i - 1][0] = train_scores[0]\n",
    "    scores[i - 1][1] = train_scores[1]\n",
    "    print(train_scores)\n",
    "    \n",
    "    print(\"Evaluation on Validation set\")\n",
    "    \n",
    "    val_scores = model.evaluate(val_x, val_y, batch_size = batch_size)\n",
    "    scores[i - 1][2] = val_scores[0]\n",
    "    scores[i - 1][3] = val_scores[1]\n",
    "    predictions = model.predict(val_x)\n",
    "    # Turn to prediction multilabel outputs\n",
    "    predictions[predictions>=0.5] = 1\n",
    "    predictions[predictions<0.5] = 0\n",
    "    #print(predictions[0])\n",
    "    # Add in hamming loss, jaccard score, recall, precision, f1 score, then save multilabel confusion matrix\n",
    "    scikit_scores = []\n",
    "    scikit_scores.append(hamming_loss(val_y, predictions))\n",
    "    scikit_scores.append(jaccard_score(val_y, predictions, average = None))\n",
    "    scikit_scores.append(recall_score(val_y, predictions, average = None))\n",
    "    scikit_scores.append(precision_score(val_y, predictions, average = None))\n",
    "    scikit_scores.append(f1_score(val_y, predictions, average = None))\n",
    "    confusion = multilabel_confusion_matrix(val_y, predictions)\n",
    "    print(confusion)\n",
    "    print(val_scores)\n",
    "    #print(scikit_scores)\n",
    "    #print(confusion)\n",
    "    \n",
    "    # Save the scikit parameters for this fold\n",
    "   # with open('10-Fold Validation/Scikit Scores For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "     #   pickle.dump(scikit_scores,f)\n",
    "    \n",
    "    # Save the confusion matrix for this fold\n",
    "    #with open('Resampling Tests/Confusion Matrix For Fold {}_Sampled_20_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "     #   pickle.dump(confusion,f)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "#np.save('10-Fold Validation/Scores_Weighted.npy', scores, allow_pickle = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
