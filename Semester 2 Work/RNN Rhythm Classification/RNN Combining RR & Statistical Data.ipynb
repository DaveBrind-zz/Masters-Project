{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Code Outline -------- #\n",
    "# Create a model that implements an RNN using output predictions from a CNN in order to identify rhythm changes.\n",
    "# In this instance we have the choice to combine either RR intervals or RR intervals as well as statistical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, multilabel_confusion_matrix, hamming_loss, jaccard_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from skmultilearn.model_selection import IterativeStratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adb de-noised/100_de-noised.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "end = len(y)\n",
    "#print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will have time series data of labels for each patient. These series will be different lengths so we need to\n",
    "# Pad them all to add 0's to the end of each sequence to get longer length inputs. So first we save how many labels are\n",
    "# In each patient as well as how many beats are in the signal.\n",
    "\n",
    "# Load in the beat labels for each patient as well as the rhythm labels for each\n",
    "\n",
    "patient_id = range(0,48)\n",
    "\n",
    "beat_labels = []\n",
    "\n",
    "beat_locations = []\n",
    "\n",
    "rhythm_labels = []\n",
    "\n",
    "rhythm_locations = []\n",
    "\n",
    "full_test = [14,24]\n",
    "half_test = [4,28,40,41,44,45]\n",
    "\n",
    "test_beat_labels = []\n",
    "test_beat_locations = []\n",
    "test_rhythm_labels = []\n",
    "test_rhythm_locations = []\n",
    "\n",
    "half_beat_labels = []\n",
    "half_beat_locations = []\n",
    "half_rhythm_labels = []\n",
    "half_rhythm_locations = []\n",
    "\n",
    "for i in patient_id:\n",
    "    # Beat labels for patient i\n",
    "    with open('adb final labels/adb beat labels/{}_beat_labels.pkl'.format(i), 'rb') as f:\n",
    "        temp_beat = pickle.load(f)\n",
    "    # beat label locations for patient i\n",
    "    with open('adb final labels/adb peaks/{}_peaks.pkl'.format(i), 'rb') as f:\n",
    "        temp_beat_locations = pickle.load(f)\n",
    "    # rhythm labels for patient i\n",
    "    with open('adb final labels/adb rhythm labels/{}_rhythm_labels.pkl'.format(i), 'rb') as f:\n",
    "        temp_rhythm = pickle.load(f)\n",
    "    # Rhythm label locations for patient i\n",
    "    with open('adb final labels/adb rhythm locations/{}_rhythm_locations.pkl'.format(i), 'rb') as f:\n",
    "        temp_rhythm_location = pickle.load(f)\n",
    "        \n",
    "    # Split into training and testing splits\n",
    "    if(i in full_test):\n",
    "        test_rhythm_locations.append(temp_rhythm_location)\n",
    "        test_beat_labels.append(temp_beat)\n",
    "        test_rhythm_labels.append(temp_rhythm)\n",
    "        test_beat_locations.append(temp_beat_locations)\n",
    "    elif(i in half_test):\n",
    "        half_rhythm_locations.append(temp_rhythm_location)\n",
    "        half_beat_labels.append(temp_beat)\n",
    "        half_rhythm_labels.append(temp_rhythm)\n",
    "        half_beat_locations.append(temp_beat_locations)\n",
    "    else:\n",
    "        rhythm_locations.append(temp_rhythm_location)\n",
    "        beat_labels.append(temp_beat)\n",
    "        rhythm_labels.append(temp_rhythm)\n",
    "        beat_locations.append(temp_beat_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(rhythm_labels))\n",
    "#print(half_rhythm_labels[-1])\n",
    "#print(len(test_rhythm_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split the half patient test data into half in order to get representative test set\n",
    "index = []\n",
    "for j in half_beat_locations:\n",
    "    # Find beat location nearest to the halfway point\n",
    "    index.append(min(enumerate(j), key=lambda x: abs(x[1]-(end/2)))[0])\n",
    "    #print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that finds the closest rhythm location to the beat location\n",
    "\n",
    "def find_beat_to_rhythm(beat_location, rhythm_location):\n",
    "    index, value = min(enumerate(rhythm_location), key=lambda x: abs(x[1]-(beat_location)))\n",
    "    return index, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have found the beat corresponding to the halfway point of the ecg, and then the function above finds the last rhythm\n",
    "# Label before this point\n",
    "\n",
    "# For every halfway beat in the list above we now need to find the nearest rhythm to it and use that as our starting rhythm\n",
    "\n",
    "for c,k in enumerate(index):\n",
    "    starting_rhythm_index, starting_rhythm_value = find_beat_to_rhythm(half_beat_locations[c][k], half_rhythm_locations[c])\n",
    "    halfway_list_locations = [end / 2]\n",
    "    halfway_list_labels = [half_rhythm_labels[c][starting_rhythm_index]]\n",
    "    \n",
    "    if(c == 0):\n",
    "        # We want left half of patient ECG in the test set\n",
    "        test_rhythm_labels.append(half_rhythm_labels[c][:starting_rhythm_index])\n",
    "        temp_list = half_rhythm_locations[c][:starting_rhythm_index] + [end / 2]\n",
    "        test_rhythm_locations.append(temp_list)\n",
    "        test_beat_labels.append(half_beat_labels[c][:k])\n",
    "        test_beat_locations.append(half_beat_locations[c][:k])\n",
    "        # Save the right half into the training set\n",
    "        temp_labels = halfway_list_labels + half_rhythm_labels[c][(starting_rhythm_index):]\n",
    "        rhythm_labels.append(temp_labels)\n",
    "        temp_locations = halfway_list_locations + half_rhythm_locations[c][(starting_rhythm_index):]\n",
    "        rhythm_locations.append(temp_locations)\n",
    "        beat_labels.append(half_beat_labels[c][k:])\n",
    "        beat_locations.append(half_beat_locations[c][k:])\n",
    "    else:\n",
    "        # We want right half of patient ECG in the test set\n",
    "        test_rhythm_labels.append(half_rhythm_labels[c][(starting_rhythm_index - 1):])\n",
    "        temp_list =  [end / 2] + half_rhythm_locations[c][starting_rhythm_index:]\n",
    "        test_rhythm_locations.append(temp_list)\n",
    "        test_beat_labels.append(half_beat_labels[c][k:])\n",
    "        test_beat_locations.append(half_beat_locations[c][k:])\n",
    "        # Save the left half into the training set\n",
    "        if(len(half_rhythm_labels[c]) == 1): # If we only have one rhythm present in the sample\n",
    "            temp_labels = [half_rhythm_labels[c][0]]\n",
    "            temp_locations = [half_rhythm_locations[c][0]]\n",
    "        else:\n",
    "            temp_labels = half_rhythm_labels[c][:starting_rhythm_index]\n",
    "            temp_locations = half_rhythm_locations[c][:starting_rhythm_index]\n",
    "        rhythm_labels.append(temp_labels)\n",
    "        rhythm_locations.append(temp_locations)\n",
    "        beat_labels.append(half_beat_labels[c][:k])\n",
    "        beat_locations.append(half_beat_locations[c][:k])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(rhythm_locations))\n",
    "#print(len(test_rhythm_locations))\n",
    "#print(test_rhythm_labels[2])\n",
    "#print(test_rhythm_locations[2])\n",
    "#print(rhythm_locations[-1])\n",
    "#print(rhythm_labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this data\n",
    "with open('Test Data/Rhythm Locations.pkl', 'wb') as f:\n",
    "    pickle.dump(test_rhythm_locations,f)\n",
    "with open('Test Data/Rhythm Labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_rhythm_labels,f)\n",
    "with open('Test Data/Beat Locations.pkl', 'wb') as f:\n",
    "    pickle.dump(test_beat_locations,f)\n",
    "with open('Test Data/Beat Labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_beat_labels,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to remove the last beat from the arrays as this is usually erroneous due to disconnection of ECG\n",
    "# Can only do this for the latter half of each training set but for the first half of patients [28,40,41,44,45] we have only\n",
    "# The left hand side of the ECG so the last labels etc are fine. We appended these on last so the last 5 patients do not need\n",
    "# Changing\n",
    "\n",
    "for i in beat_locations:\n",
    "    del i[-1]\n",
    "for j in beat_labels:   \n",
    "    del j[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#beat_locations[6][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the starting and ending points of all the rhythms, we need to create windows of pre-determined\n",
    "# Sample size and then these will be given a target label in the form of a vector. This vector will tell us which \n",
    "# Rhythms are present within the sample.\n",
    "\n",
    "# Choose a sample size - The number of beat labels you are going to send in\n",
    "sample_size = 10\n",
    "\n",
    "# Now we need to create sublists for each patient with 10 beat labels in\n",
    "full_list = []\n",
    "for j in beat_labels:\n",
    "    samples = [j[i * sample_size:(i + 1) * sample_size] for i in range((len(j) + sample_size - 1) // sample_size )]\n",
    "    full_list.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#print(len(full_list[0]))\n",
    "#print(full_list[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to replace each type with their corresponding integer number\n",
    "integer_samples = [[[0 if b == 'N'\n",
    "          else 1 if b == 'L'\n",
    "          else 2 if b == 'R'\n",
    "          else 3 if b == 'A'\n",
    "          else 4 if b == 'a'\n",
    "          else 5 if b == 'J'\n",
    "          else 6 if b == 'S'\n",
    "          else 7 if b == 'V'\n",
    "          else 8 if b == 'F'\n",
    "          else 9 if b == '!'\n",
    "          else 10 if b == 'e'\n",
    "          else 11 if b == 'j'\n",
    "          else 12 if b == 'E'\n",
    "          else 13 if b == '/'\n",
    "          else 14 if b == 'f'\n",
    "          else 15 if b == 'x'\n",
    "          else 16 if b == 'Q'\n",
    "          else 17 for b in j] for j in k] for k in full_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#print(integer_samples[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#print(full_list[2][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now some of the samples will be missing beats so for these we need to pad them with arbitrary values \n",
    "# To make them 10 in length\n",
    "\n",
    "# Loop over the samples and pad the last elements\n",
    "for i in integer_samples:\n",
    "    for c,j in enumerate(i):\n",
    "        if (c == (len(i) - 1)):\n",
    "            # Pad last sample\n",
    "            element_to_add = sample_size - len(j)\n",
    "            for k in range(element_to_add):\n",
    "                new = [99]\n",
    "                j = j + new\n",
    "            i[c] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "#print(integer_samples[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have loads of samples in integer format with the encoding from above. We now need to go through and\n",
    "# Create new vector label arrays of dimension (15,) in form [1,0,0,0.....] if it is a Atrial bigeminy\n",
    "# [0,1,0,0,0,0...] if it is a Atrial fibrillation etc. If the sample contains a mixture of rhythms then we opt for\n",
    "# A label such as [1,1,0,0,0....] for AF and atrial bigeminy\n",
    "\n",
    "# Now we need to create sublists for each patient with 25 beat label locations in\n",
    "location_list = []\n",
    "for j in beat_locations:\n",
    "    samples = [j[i * sample_size:(i + 1) * sample_size] for i in range((len(j) + sample_size - 1) // sample_size )]\n",
    "    location_list.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rhythm_locations[-2])\n",
    "#print(location_list[-1][-1])\n",
    "#print(rhythm_labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to loop over every sample and check where the beat labels fit into with regards to the rhythm ranges\n",
    "# For ease we will assume the first few samples up the rhythm label are also the same rhythm.\n",
    "# Rhythm labels:\n",
    "# AB - atrial bigeminy (0), AFIB - atrial fibrillation(1), AFL - atrial flutter(2), B - ventricular bigeminy(3)\n",
    "# BII - 2 heart block(4), IVR - idioventricular rhythm(5), N - normal sinus rhythm(6), NOD - nodal rhythm(7)\n",
    "# P - paced rhythm(8), PREX - pre-excitation(9), SBR - sinus brachycardia(10), SVTA - supraventricular tachyarrhymia(11)\n",
    "# T - ventricular trigeminy(12), VFL - ventricular flutter(13), VT - ventricular tachycardia(14)\n",
    "\n",
    "sample_labels = []\n",
    "\n",
    "# First pick a patient\n",
    "for i,patient in enumerate(rhythm_locations):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    patient_beat_labels = []\n",
    "    \n",
    "    # Now loop over all the beat labels in that patients data\n",
    "    for beat_location in beat_locations[i]:\n",
    "        \n",
    "        # Loop over all the rhythms and find which one it is after and use that rhythm label\n",
    "        \n",
    "        rhythm_after = []\n",
    "        \n",
    "        for c,rhythm_location in reversed(list(enumerate(patient))):\n",
    "            if (len(rhythm_after) > 0):\n",
    "                break\n",
    "            else:\n",
    "                if(beat_location > rhythm_location):\n",
    "                    rhythm_after.append(rhythm_labels[i][c])\n",
    "                    #print(rhythm_labels[i][c])\n",
    "                \n",
    "        patient_beat_labels.append(rhythm_after)\n",
    "        #print(patient_beat_labels[-1])\n",
    "       # print(beat_location)\n",
    "        #print(i)\n",
    "        \n",
    "    sample_labels.append(patient_beat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sample_labels[-1])\n",
    "#print(len(beat_locations[8]))\n",
    "#print(beat_locations[8][90:110])\n",
    "#print(rhythm_locations[8])\n",
    "#print(rhythm_labels[8][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the rhythms set up we need to segment into blocks of sample_size again\n",
    "\n",
    "full_list_labels = []\n",
    "for j in sample_labels:\n",
    "    samples_labels = [j[i * sample_size:(i + 1) * sample_size] for i in range((len(j) + sample_size - 1) // sample_size )]\n",
    "    full_list_labels.append(samples_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "# Per patient\n",
    "for c,i in enumerate(full_list_labels):\n",
    "    print(c)\n",
    "    patient = []\n",
    "    # Each sample\n",
    "    for j in i:\n",
    "        new = []\n",
    "        # Get rid of annoying list notation\n",
    "        for k in j:\n",
    "            try:\n",
    "                new.append(k[0])\n",
    "            except IndexError:\n",
    "                print(k)\n",
    "            #new.append(k[0])\n",
    "        patient.append(new)\n",
    "    x.append(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x[-2][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(integer_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in an array of sample_labels as well as a boolean ~(tells us if there is a mixture or not)~\n",
    "# We then create a corresponding vector for that sample\n",
    "def create_complete_label(sample_labels):\n",
    "\n",
    "    # Doing this finds the UNIQUE elements of the sample_labels\n",
    "    unique_elements = list(set(sample_labels))\n",
    "    \n",
    "    #print(unique_elements)\n",
    "    #for i in unique_elements:\n",
    "        #print(i)\n",
    "        \n",
    "    # Create vector - some reason they have a bracket in front of all of them when you extract them\n",
    "    label = [0 if b == 'AB'\n",
    "        else 1 if b == 'AFIB'\n",
    "        else 2 if b == 'AFL'\n",
    "        else 3 if b == 'B'\n",
    "        else 4 if b == 'BII'\n",
    "        else 5 if b == 'IVR'\n",
    "        else 6 if b == 'N'\n",
    "        else 7 if b == 'NOD'\n",
    "        else 8 if b == 'P'\n",
    "        else 9 if b == 'PREX'\n",
    "        else 10 if b == 'SBR'\n",
    "        else 11 if b == 'SVTA'\n",
    "        else 12 if b == 'T'\n",
    "        else 13 if b == 'VFL'\n",
    "        # This one is VT label\n",
    "        else 14 for b in unique_elements]\n",
    "        \n",
    "    # Return the vector\n",
    "    return(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_labels = []\n",
    "for i in x:\n",
    "    patient_l = []\n",
    "    for j in i:\n",
    "        patient_l.append(create_complete_label(j))\n",
    "        \n",
    "    complete_labels.append(patient_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(complete_labels[-1])\n",
    "#print(rhythm_labels[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to transfer these into vectors like [1,0,0,0,0,0,0,0,0,0...] if label 1 is present or\n",
    "# [1,1,0,0,0,0,0,0,0....] if a mixture of label 1 or 2 are present etc.\n",
    "\n",
    "y = []\n",
    "\n",
    "# Loop over every sample and create a vector for it and append it to a list\n",
    "for c,patient in enumerate(complete_labels):\n",
    "    \n",
    "    for sample in patient:\n",
    "        \n",
    "        # Create a label vector\n",
    "        temp = np.zeros((15,))\n",
    "        \n",
    "        # Find which numbers are present, then these indices need to be set to 1\n",
    "        for item in sample:\n",
    "            temp[item] = 1\n",
    "            \n",
    "        y.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have complete set of target labels and input arrays\n",
    "# Check\n",
    "#print(integer_samples[0])\n",
    "# Need to convert integer_samples to a 1 dimensional array of 25 sample windows\n",
    "#print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in integer_samples:\n",
    "    for j in i:\n",
    "        x.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(len(y), sample_size,1)\n",
    "#print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- SECTION FOR RR INTERVAL ADDITION -------- #\n",
    "# See what effect adding RR interval lengths will do if we use it as a second feature.\n",
    "# Each peak will take a value for its RR length to then next peak, except the last one which will just\n",
    "# Be set to 0\n",
    "\n",
    "# First split each patients beats into chunks of sample size\n",
    "patient_sizes = []\n",
    "new_locations = []\n",
    "for k in range(len(beat_locations)):\n",
    "    \n",
    "    patient_sizes.append(len(beat_locations[k]) / sample_size)\n",
    "    \n",
    "    new_locations.append([beat_locations[k][i:i + sample_size] for i in range(0, len(beat_locations[k]), sample_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now find the distance between all the R peaks within that window and set the very final one to 0 so it is same length\n",
    "def distance(point1, point2):\n",
    "    return(abs(point1 - point2))\n",
    "\n",
    "RR_distances = []\n",
    "for patient in new_locations:\n",
    "    patient_RR = []\n",
    "    for c,sample in enumerate(patient):\n",
    "        temp = []\n",
    "        \n",
    "        if (c != len(patient) - 1):\n",
    "            for i in range(0, sample_size, 1):\n",
    "                if(i == sample_size - 1):\n",
    "                    temp += [0]\n",
    "                else:    \n",
    "                    temp += [distance(sample[i],sample[i+1])]\n",
    "        \n",
    "        # Last set for patient will not be a complete set of sample size so work out what is in there to do then pad with 0's\n",
    "        else:\n",
    "        \n",
    "            number_peaks = len(sample)\n",
    "            \n",
    "            number_to_pad = sample_size - number_peaks\n",
    "            \n",
    "            for i in range(0, number_peaks, 1):\n",
    "                if(i == number_peaks - 1):\n",
    "                    temp += [0]\n",
    "                else:\n",
    "                    temp += [distance(sample[i],sample[i+1])]\n",
    "                    \n",
    "            for j in range(number_to_pad):\n",
    "                temp += [0]\n",
    "        \n",
    "        patient_RR.append(temp)\n",
    "    RR_distances += patient_RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hrvanalysis\n",
    "from hrvanalysis.extract_features import get_time_domain_features,get_frequency_domain_features,get_geometrical_features, get_poincare_plot_features, get_sampen\n",
    "from scipy.stats import entropy\n",
    "\n",
    "RR_distances = RR_distances.reshape((RR_distances.shape[0], sample_size))\n",
    "\n",
    "# Find different statistical information about the RR time sequence evolution. These are in the form of time domains,\n",
    "# Frequency Domains as well as non-linear features\n",
    "\n",
    "indexes_to_remove = []\n",
    "mean_nni = []\n",
    "sdnn = []\n",
    "sdsd = []\n",
    "pnni_20 = []\n",
    "pnni_50 = []\n",
    "range_nni = []\n",
    "total_power = []\n",
    "lf = []\n",
    "hf = []\n",
    "lf_hf_ratio = []\n",
    "sd1 = []\n",
    "sample_entropy = []\n",
    "\n",
    "# Calculate it for each window in the RR distances matrix\n",
    "\n",
    "for c,segment in enumerate(RR_distances):\n",
    "    \n",
    "    #print(c/len(RR_distances) * 100)\n",
    "    \n",
    "    # Make a copy so we do not adjust the RR distances matrix accidentally\n",
    "    temp = segment.copy()\n",
    "    # Only interested in the values which are not 0 (ie the paddings)\n",
    "    temp = list(filter(lambda a: a != 0, temp))\n",
    "    \n",
    "    # If we can only get one RR interval then we are at the last few peaks in the patients ECG signal, We should save this indices\n",
    "    # And just remove them from the sets, the sample entropy requires minimum 3 RR intervals so remove any less than that\n",
    "    if(len(temp) <= 3):\n",
    "        indexes_to_remove.append(c)\n",
    "        continue\n",
    "    \n",
    "    # Now loop over the temporary segment and find the time series evolution of the means\n",
    "    \n",
    "    time_series_mean_nni = []\n",
    "    time_series_sdnn = []\n",
    "    time_series_sdsd = []\n",
    "    time_series_pnni_20 = []\n",
    "    time_series_pnni_50 = []\n",
    "    time_series_range_nni = []\n",
    "    time_series_total_power = []\n",
    "    time_series_lf = []\n",
    "    time_series_hf = []\n",
    "    time_series_lf_hf_ratio = []\n",
    "    time_series_sd1 = []\n",
    "    time_series_sample_entropy = []\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        \n",
    "        if(i == 0):\n",
    "            \n",
    "            time_series_mean_nni.append(0)\n",
    "            time_series_sdnn.append(0)\n",
    "            time_series_sdsd.append(0)\n",
    "            time_series_pnni_20.append(0)\n",
    "            time_series_pnni_50.append(0)\n",
    "            time_series_range_nni.append(0)\n",
    "            time_series_total_power.append(0)\n",
    "            time_series_lf.append(0)\n",
    "            time_series_hf.append(0)\n",
    "            time_series_lf_hf_ratio.append(0)\n",
    "            time_series_sd1.append(0)\n",
    "            time_series_sample_entropy.append(0)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            temp_RR = temp[:(i+1)]\n",
    "            \n",
    "            #print(temp_RR)\n",
    "            \n",
    "            # Now find the features and extract the ones we require\n",
    "            # For time domain we will use Mean,SDNN,RMSSD,SDSD, pNN20, pNN50\n",
    "            # Frequency domain we will use totalPSD, HF\n",
    "            # Non linear features we will use SD1/SD2 and sample entropy\n",
    "\n",
    "            time_domain = get_time_domain_features(temp_RR)\n",
    "\n",
    "            # Time domain components\n",
    "            time_series_mean_nni.append(time_domain['mean_nni'])\n",
    "            time_series_sdnn.append(time_domain['sdnn'])\n",
    "            time_series_sdsd.append(time_domain['sdsd'])\n",
    "            time_series_pnni_20.append(time_domain['pnni_20'])\n",
    "            time_series_pnni_50.append(time_domain['pnni_50'])\n",
    "            time_series_range_nni.append(time_domain['range_nni'])\n",
    "\n",
    "            frequency_domain = get_frequency_domain_features(temp_RR, 'welch', 360)\n",
    "\n",
    "            # Frequency domain components\n",
    "            time_series_total_power.append(frequency_domain['total_power'])\n",
    "            time_series_hf.append(frequency_domain['hf'])\n",
    "            \n",
    "            # Non linear features require minimum 3 RR intervals\n",
    "            \n",
    "            if (i  == 1):\n",
    "                \n",
    "                time_series_sd1.append(0)\n",
    "                time_series_sample_entropy.append(0)\n",
    "                \n",
    "            else:\n",
    "\n",
    "                poincare = get_poincare_plot_features(temp_RR)\n",
    "\n",
    "                # Get non linear features\n",
    "                time_series_sd1.append(poincare['sd1'])\n",
    "\n",
    "                RR_entropy = entropy(temp_RR)\n",
    "\n",
    "                time_series_sample_entropy.append(RR_entropy)\n",
    "        \n",
    "    mean_nni.append(time_series_mean_nni)\n",
    "    sdnn.append(time_series_sdnn)\n",
    "    sdsd.append(time_series_sdsd)\n",
    "    pnni_20.append(time_series_pnni_20)\n",
    "    pnni_50.append(time_series_pnni_50)\n",
    "    range_nni.append(time_series_range_nni)\n",
    "    total_power.append(time_series_total_power)\n",
    "    hf.append(time_series_hf)\n",
    "    sd1.append(time_series_sd1)\n",
    "    sample_entropy.append(time_series_sample_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fit these all together and normalise\n",
    "from sklearn.preprocessing import normalize\n",
    "mean_nni = np.array(mean_nni)\n",
    "mean_nni = normalize(mean_nni)\n",
    "sdnn = np.array(sdnn)\n",
    "sdnn = normalize(sdnn)\n",
    "sdsd = np.array(sdsd)\n",
    "sdsd = normalize(sdsd)\n",
    "pnni_20 = np.array(pnni_20)\n",
    "pnni_20 = normalize(pnni_20)\n",
    "pnni_50 = np.array(pnni_50)\n",
    "pnni_50 = normalize(pnni_50)\n",
    "range_nni = np.array(range_nni)\n",
    "range_nni = normalize(range_nni)\n",
    "total_power = np.array(total_power)\n",
    "total_power = normalize(total_power)\n",
    "hf = np.array(hf)\n",
    "hf = normalize(hf)\n",
    "sd1 = np.array(sd1)\n",
    "sd1 = normalize(sd1)\n",
    "sample_entropy = np.array(sample_entropy)\n",
    "sample_entropy = normalize(sample_entropy)\n",
    "\n",
    "#print(sample_entropy.shape)\n",
    "#print(mean_nni[0])\n",
    "#print(sdnn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(RR_distances[0])\n",
    "#print(x[0])\n",
    "#print(SVM_RR_distances[0])\n",
    "#print(svm_x[0])\n",
    "\n",
    "# Remove the windows which we could not use as they were less than 3 in length\n",
    "svm_x = np.delete(x, indexes_to_remove)\n",
    "SVM_RR_distances = np.delete(RR_distances, indexes_to_remove)\n",
    "svm_y = np.delete(y, indexes_to_remove)\n",
    "\n",
    "# Reshape them\n",
    "svm_x = svm_x.reshape((svm_x.shape[0], svm_x.shape[1]))\n",
    "SVM_RR_distances = SVM_RR_distances.reshape((SVM_RR_distances.shape[0],SVM_RR_distances.shape[1]))\n",
    "\n",
    "# Normalise the RR intervals\n",
    "SVM_RR_distances = normalize(SVM_RR_distances)\n",
    "\n",
    "#print(svm_x[0])\n",
    "#print(svm_x.shape)\n",
    "#print(SVM_RR_distances.shape)\n",
    "#print(SVM_RR_distances[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now stack on the statistics features\n",
    "total_x = np.stack([svm_x,SVM_RR_distances,mean_nni, sdnn, sdsd, pnni_20, pnni_50, range_nni, total_power, hf, sd1, sample_entropy], axis = 2)\n",
    "#print(total_x.shape)\n",
    "#print(total_x[0])\n",
    "#print(svm_y.shape)\n",
    "#print(svm_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define a function that finds proportions of the different labels\n",
    "\n",
    "def proportions(labels):\n",
    "    \n",
    "    # Data is in vector form so [1,0,0,0,0....] etc with this indicating it is the first rhythm\n",
    "    # We need to loop over the vector to find where the 1's are then we can get the proportion that is a certain type\n",
    "    \n",
    "    n_mixed, n_pure, n_AB, n_AFIB, n_AFL, n_B, n_BII, n_IVR, n_N, n_NOD, n_P, n_PREX, n_SBR, n_SVTA, n_T, n_VFL, n_VT = 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "    \n",
    "    options = {0 : n_AB,\n",
    "           1 : n_AFIB,\n",
    "           2 : n_AFL,\n",
    "           3 : n_B,\n",
    "           4 : n_BII,\n",
    "           5 : n_IVR,\n",
    "           6 : n_N,\n",
    "           7 : n_NOD,\n",
    "           8 : n_P,\n",
    "           9 : n_PREX,\n",
    "          10 : n_SBR,\n",
    "          11 : n_SVTA,\n",
    "          12 : n_T,\n",
    "          13 : n_VFL,\n",
    "          14 : n_VT,\n",
    "          15 : n_mixed,\n",
    "          16 : n_pure\n",
    "}\n",
    "\n",
    "    for i in labels:\n",
    "        #print(i)\n",
    "        mix = False\n",
    "        count = 0\n",
    "        for index,number in enumerate(i):\n",
    "            #print(count)\n",
    "            #print(number)\n",
    "            \n",
    "            #if ((number == 1) and (index == 0)):\n",
    "                #print(i)\n",
    "            \n",
    "            if(number == 1):\n",
    "                count += 1\n",
    "            \n",
    "                # Change variable in dictionary to one up\n",
    "                options[index] += 1\n",
    "                \n",
    "            elif(number == 0):\n",
    "                count = count\n",
    "        \n",
    "        if(count >= 2):\n",
    "                mix = True\n",
    "                options[15] += 1\n",
    "        elif(count == 1):\n",
    "                mix = False\n",
    "                options[16] += 1\n",
    "                \n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = proportions(svm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_split)\n",
    "\n",
    "# This checks we have correct number of samples\n",
    "#print(data_split[15] + data_split[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training data and training labels\n",
    "#with open('RNN Training Data.pkl', 'wb') as f:\n",
    " #   pickle.dump(x, f)\n",
    "#with open('RNN Training Targets.pkl', 'wb') as f:\n",
    " #   pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As some labels are not present frequently in the sample, we need to implement a k fold validation scheme\n",
    "# In the hope that this will produce a more representative set.\n",
    "\n",
    "import math\n",
    "\n",
    "batch_size = 28\n",
    "\n",
    "# Many thanks to creators of iterative stratification and scikit-multilearn\n",
    "# Reference :\n",
    "# If you use this method to stratify data please cite both:\n",
    "# 1 -> Sechidis, K., Tsoumakas, G., & Vlahavas, I. (2011). On the stratification of multi-label data. Machine Learning and Knowledge Discovery in Databases, 145-158. http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf\n",
    "# 2 -> Piotr Szymański, Tomasz Kajdanowicz ; Proceedings of the First International Workshop on Learning with Imbalanced Domains: Theory and Applications, PMLR 74:22-35, 2017. http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html\n",
    "# Found on http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html\n",
    "\n",
    "\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    " \n",
    "# The more splits you do the more likely the data is representative however this comes with trade off that\n",
    "# You will have a lower percentage of data in your test set as well as the fact that it will take significantly\n",
    "# Longer computationally. Thankfully the RNN is very fast as we are just using sequences so a 10-fold validation\n",
    "# Will be sufficient and leaves 90% of the data for training purposes\n",
    "\n",
    "n_split = 10\n",
    " \n",
    "k_fold = IterativeStratification(n_splits = n_split, order=1)\n",
    "\n",
    "i = 1\n",
    "\n",
    "# This is just to check that splits are somewhat correct and then crucially saves the data set splits for each fold\n",
    "# Into a file for analysis later\n",
    "\n",
    "# Save the training and validation indices for each fold so that we can use them on the server\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for train, val in k_fold.split(total_x, svm_y):\n",
    "    print(\"Fold\" + str(i))\n",
    "    temp_1 = proportions(y[train])\n",
    "    temp_2 = proportions(y[val])\n",
    "    print(\"Train set\")\n",
    "    print(temp_1)\n",
    "    print(\"Validation set\")\n",
    "    print(temp_2)\n",
    "    i += 1\n",
    "    train_indices.append(train)\n",
    "    val_indices.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save the indices for GPU use on server\n",
    "#with open('RNN Training Indices.pkl', 'wb') as f:\n",
    " #   pickle.dump(train_indices, f)\n",
    "#with open('RNN Validation Indices.pkl', 'wb') as f:\n",
    " #   pickle.dump(val_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a weighted loss that will account for the multilabel data being imbalanced\n",
    "def calculating_class_weights(y_true):\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    number_dim = np.shape(y_true)[1]\n",
    "    weights = np.empty([number_dim, 2])\n",
    "    for i in range(number_dim):\n",
    "        weights[i] = compute_class_weight('balanced', [0.,1.], y_true[:, i])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a loss function that accounts for these weights for multilabel data\n",
    "\n",
    "import keras.backend as K\n",
    "def get_weighted_loss(weights):\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data folds are ready to go so now we need to train on every fold and save a couple of things:\n",
    "# 1. Data set for each fold\n",
    "# 2. Actual data for each fold\n",
    "# 3. Accuracies on validation and training set for each fold\n",
    "# 4. Average accuracy and loss over 10 folds\n",
    "\n",
    "i = 1\n",
    "\n",
    "# Need to save the scores for each fold evaluation\n",
    "scores = np.zeros((n_split, 4))\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "for train, val in k_fold.split(total_x, svm_y):\n",
    "    \n",
    "    print(\"Fold \" + str(i))\n",
    "    \n",
    "    train_x = total_x[train]\n",
    "    train_y = svm_y[train]\n",
    "    val_x = total_x[val]\n",
    "    val_y = svm_y[val]\n",
    "    \n",
    "    #class_weights = calculating_class_weights(train_y)\n",
    "    \n",
    "    # Find new proportions and save them\n",
    "    total = []\n",
    "    data_split_train = proportions(train_y)\n",
    "    total.append(data_split_train)\n",
    "    data_split_val = proportions(val_y)\n",
    "    total.append(data_split_val)\n",
    "    \n",
    "    with open('RNN RR Stats 10 Fold Validation/Data Splits For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "        pickle.dump(total,f)\n",
    "    \n",
    "    #train_x,train_y = fit_to_batch(train_x,train_y,batch_size)\n",
    "    #val_x, val_y = fit_to_batch(val_x,val_y,batch_size)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units = 256,\n",
    "                            stateful = False,\n",
    "                            recurrent_dropout = 0.2,\n",
    "                            activation = 'sigmoid'),\n",
    "                            input_shape = (sample_size,total_x.shape[-1])))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(15, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Fit to data\n",
    "    \n",
    "    history = model.fit(train_x, train_y, verbose = 1, batch_size = batch_size, validation_data = (val_x, val_y), epochs = 50, shuffle = True)\n",
    "    \n",
    "    # Save history for each fold\n",
    "    with open('RNN RR Stats 10 Fold Validation/History For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "        pickle.dump(history,f)\n",
    "    \n",
    "    # Save the evaluate values for later\n",
    "    print(\"Evaluation on Training set\")\n",
    "    \n",
    "    train_scores = model.evaluate(train_x, train_y, batch_size = batch_size)\n",
    "    scores[i - 1][0] = train_scores[0]\n",
    "    scores[i - 1][1] = train_scores[1]\n",
    "    #print(train_scores)\n",
    "    \n",
    "    print(\"Evaluation on Validation set\")\n",
    "    \n",
    "    val_scores = model.evaluate(val_x, val_y, batch_size = batch_size)\n",
    "    scores[i - 1][2] = val_scores[0]\n",
    "    scores[i - 1][3] = val_scores[1]\n",
    "    predictions = model.predict(val_x)\n",
    "    # Turn to prediction multilabel outputs\n",
    "    predictions[predictions>=0.5] = 1\n",
    "    predictions[predictions<0.5] = 0\n",
    "    #print(predictions[0])\n",
    "    # Add in hamming loss, jaccard score, recall, precision, f1 score, then save multilabel confusion matrix\n",
    "    scikit_scores = []\n",
    "    scikit_scores.append(hamming_loss(val_y, predictions))\n",
    "    scikit_scores.append(jaccard_score(val_y, predictions, average = None))\n",
    "    scikit_scores.append(recall_score(val_y, predictions, average = None))\n",
    "    scikit_scores.append(precision_score(val_y, predictions, average = None))\n",
    "    scikit_scores.append(f1_score(val_y, predictions, average = None))\n",
    "    confusion = multilabel_confusion_matrix(val_y, predictions)\n",
    "    print(confusion)\n",
    "    print(f1_score(val_y, predictions, average = None))\n",
    "    #print(val_scores)\n",
    "    #print(scikit_scores)\n",
    "    #print(confusion)\n",
    "    \n",
    "    # Save the scikit parameters for this fold\n",
    "    with open('RNN RR Stats 10 Fold Validation/Scikit Scores For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "        pickle.dump(scikit_scores,f)\n",
    "    \n",
    "    # Save the confusion matrix for this fold\n",
    "    with open('RNN RR Stats 10 Fold Validation/Confusion Matrix For Fold {}_Weighted.pkl'.format(str(i)), 'wb') as f:\n",
    "        pickle.dump(confusion,f)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "np.save('RNN RR Stats 10 Fold Validation/Scores_Weighted.npy', scores, allow_pickle = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
